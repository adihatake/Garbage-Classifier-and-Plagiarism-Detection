{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adi NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adihatake/garbage_classifier/blob/master/Adrian's%20Cheat%20Detect%20Code%20NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZO33oNuxDed"
      },
      "source": [
        "# **Adrian Balajadia's Cheat Detect Source Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCnWYPaaYMY1"
      },
      "source": [
        "# Import essential modules and libraries\n",
        "\n",
        "In my program, notable software used were:\n",
        "\n",
        "- Natural Language Processing Toolkit (NLTK)\n",
        "- Gensim\n",
        "- Matplotlib\n",
        "- Pandas\n",
        "- Numpy\n",
        "- Scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6MpKA2zkk6Z"
      },
      "source": [
        "#import the dependencies\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('tagsets')\n",
        "\n",
        "import math\n",
        "import builtins \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, string, unicodedata\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import inflect\n",
        "import string  \n",
        "import sklearn\n",
        "import collections\n",
        "import re\n",
        "import gensim\n",
        "import operator\n",
        "import itertools\n",
        "import heapq\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import string  \n",
        "from heapq import nsmallest, nlargest\n",
        "from collections import Counter\n",
        "\n",
        "from operator import sub\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from gensim import corpora, models\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "from gensim.summarization.summarizer import summarize\n",
        "from gensim.summarization import keywords\n",
        "from gensim.models import HdpModel, CoherenceModel\n",
        "\n",
        "from nltk import pos_tag, pos_tag_sents\n",
        "from nltk import edit_distance\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, LancasterStemmer\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from pprint import pprint\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "Doc_tokens = {}\n",
        "\n",
        "Function_Words = ['the', 'and', 'a', 'of', 'to', 'in', 'that', 'with', 'as', 'it',\n",
        "'for', 'but', 'at', 'on', 'this', 'all', 'by', \n",
        "'which', 'they', 'so', 'from', 'no', 'or', 'one', \n",
        "'what', 'if', 'an', 'would', 'when', 'will']\n",
        "\n",
        "Accused = {}\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-F8VuXY09jJ"
      },
      "source": [
        "# Define Preprocessing Functions\n",
        "This included:\n",
        "\n",
        "- Stop word removal (e.g. the, a, and)\n",
        "- Removal of punctuation\n",
        "- Removal of accents\n",
        "- Lowercasing all words\n",
        "- Removal of numbers\n",
        "- Lemmatization: In this function, the program changes all terms to their base form in respect of what type of word it is.\n",
        "\n",
        "\n",
        "\n",
        "Code in this cell was burrowed from here:\n",
        "Mayo, Matthew. “Text Data Preprocessing: A Walkthrough in Python.” KDnuggets, 2018, www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhVmB2vb0cUV"
      },
      "source": [
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all integer occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def lemmatize_adj(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized adj\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas_adj = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='a')\n",
        "        lemmas_adj.append(lemma)\n",
        "    return lemmas_adj  \n",
        "\n",
        "def lemmatize_nouns(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized adj\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas_nouns = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='n')\n",
        "        lemmas_nouns.append(lemma)\n",
        "    return lemmas_nouns  \n",
        "\n",
        "def lemmatize_adverbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized adj\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas_adverbs = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='r')\n",
        "        lemmas_adverbs.append(lemma)\n",
        "    return lemmas_adverbs  \n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "\n",
        "def preprocess(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = replace_numbers(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = lemmatize_verbs(words)\n",
        "    words = lemmatize_adj(words)\n",
        "    words = lemmatize_adverbs(words)\n",
        "    words = lemmatize_nouns(words)\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blRlI0MBTRuQ"
      },
      "source": [
        "# Define Frequency Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zQdhVAEOm_C"
      },
      "source": [
        "def POS_Freq(num):\n",
        " # print(Feature_Data[num])\n",
        "  Most_common_trigrams = []\n",
        "  TrigramPOS_dict = {}\n",
        "\n",
        "  text = pos_tag(nltk.word_tokenize(Feature_Data[num]))\n",
        "  POS_tags = [item[1] for item in text]\n",
        "  output = list(nltk.trigrams(POS_tags))\n",
        "\n",
        "  for trigrams in output:\n",
        "    POS_freq = output.count(trigrams)\n",
        "\n",
        "    if trigrams in TrigramPOS_dict:\n",
        "      Value = TrigramPOS_dict.get(trigrams)\n",
        "      Value = Value + 1\n",
        "      TrigramPOS_dict[trigrams] = Value\n",
        "      \n",
        "    else: \n",
        "      TrigramPOS_dict[trigrams] = POS_freq\n",
        "\n",
        "  Commons1 = Counter(TrigramPOS_dict)\n",
        "  Commons2 = Commons1.most_common()\n",
        "\n",
        "  Most_common_trigrams2 = [POS_trigrams[0] for POS_trigrams in Commons2]\n",
        "  \n",
        "  Most_common_trigrams.append(Most_common_trigrams2[0])\n",
        "  Most_common_trigrams.append(Most_common_trigrams2[1])\n",
        "  Most_common_trigrams.append(Most_common_trigrams2[2])\n",
        "  Most_common_trigrams.append(Most_common_trigrams2[3])\n",
        "\n",
        "\n",
        "  return Most_common_trigrams\n",
        "\n",
        "\n",
        "\n",
        "def Word_Trigrams(num):\n",
        "  text = nltk.word_tokenize(Feature_Data[num])\n",
        "  output = list(nltk.trigrams(text))\n",
        "  \n",
        "  Trigram_Word_list.append(output)\n",
        "\n",
        "\n",
        "def Punctuation_Freq(num):\n",
        "  Punctuation_freq2 = {}\n",
        "  Punctuation_freq = {}\n",
        "  Final_Punctuation_freq = {}\n",
        "  Punctuation_values = None\n",
        "  text = nltk.word_tokenize(Feature_Data[num])\n",
        "  \n",
        "  for character in text:  \n",
        "    if character in string.punctuation: \n",
        "\n",
        "      if character in Punctuation_freq:\n",
        "        Value = Punctuation_freq.get(character)\n",
        "        Value = Value + 1\n",
        "        Punctuation_freq[character] = Value\n",
        "\n",
        "      else: \n",
        "        Punctuation_freq[character] = 1\n",
        "\n",
        "  N = 5\n",
        "\n",
        "  Top_5 = nlargest(N, Punctuation_freq, key = Punctuation_freq.get) \n",
        "    \n",
        "  for val in Top_5:\n",
        "    key = Punctuation_freq.get(val)\n",
        "    Final_Punctuation_freq[val] = key\n",
        "\n",
        "  return Final_Punctuation_freq\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "def FunctionWord_freq(num):\n",
        "  Function_word_freq2 = {}\n",
        "  Function_word_freq = {}\n",
        "  Final_Function_word_freq = {}\n",
        "  Function_values = None\n",
        "  text = nltk.word_tokenize(Feature_Data[num])\n",
        "    \n",
        "  for word in text:  \n",
        "    if word in Function_Words: \n",
        "      if word in Function_word_freq:\n",
        "        Value = Function_word_freq.get(word)\n",
        "        Value = Value + 1\n",
        "        Function_word_freq[word] = Value\n",
        "\n",
        "      else: \n",
        "        Function_word_freq[word] = 1\n",
        "\n",
        "  N = 5\n",
        "\n",
        "  Top_5 = nlargest(N, Function_word_freq, key = Function_word_freq.get) \n",
        "    \n",
        "  for val in Top_5:\n",
        "    key = Function_word_freq.get(val)\n",
        "    Final_Function_word_freq[val] = key\n",
        "\n",
        "  return Final_Function_word_freq\n",
        "\n",
        "\n",
        "\n",
        "def NumberOfSents(num):\n",
        "  text = nltk.sent_tokenize(Feature_Data[num])\n",
        "\n",
        "  numOfsents = len(text)\n",
        "\n",
        "  return numOfsents\n",
        "\n",
        "\n",
        "\n",
        "def NumberOfWords(num):\n",
        "  text = nltk.word_tokenize(Feature_Data[num])\n",
        "\n",
        "  numOfwords = len(text)\n",
        "\n",
        "  return numOfwords\n",
        "\n",
        "\n",
        "\n",
        "def Standard_Deviation_Of_Sents(num):\n",
        "  length_of_sentences = []\n",
        "  Mean = []\n",
        "  Standard_deviation = []\n",
        "  List = []\n",
        "  Differences = []\n",
        "\n",
        "  sentences = nltk.sent_tokenize(Feature_Data[num])\n",
        "\n",
        "  for sents in sentences:\n",
        "      tokens = nltk.word_tokenize(sents)\n",
        "      sent_length = len(tokens)\n",
        "      length_of_sentences.append(sent_length)\n",
        "\n",
        "\n",
        "  for lenOfSents in length_of_sentences:\n",
        "      \n",
        "    if len(Mean) == 0:\n",
        "      Mean.append(lenOfSents)\n",
        "      List.append(lenOfSents)\n",
        "    else:\n",
        "      Mean[0] = Mean[0] + lenOfSents\n",
        "\n",
        "\n",
        "  Mean[0] = Mean[0] / len(length_of_sentences)\n",
        "\n",
        "  for numbers in length_of_sentences:\n",
        "    numbers = numbers - Mean[0]\n",
        "    numbers = numbers ** 2\n",
        "    Differences.append(numbers)\n",
        "\n",
        "\n",
        "  for items in Differences:\n",
        "    if len(Standard_deviation) == 0:\n",
        "      Standard_deviation.append(items)\n",
        "\n",
        "    else:\n",
        "      Standard_deviation[0] = Standard_deviation[0] + items\n",
        "\n",
        "\n",
        "  Standard_deviation[0] = Standard_deviation[0] / len(length_of_sentences)\n",
        "\n",
        "  Standard_deviation[0] = math.sqrt(Standard_deviation[0])\n",
        "\n",
        "  Standard_deviation = list(Standard_deviation)\n",
        "    \n",
        "  Mean = list(Mean)\n",
        "\n",
        "  return Standard_deviation, Mean\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def AverageSentLength(num):\n",
        "  Average_length2 = []\n",
        "\n",
        "  text = nltk.sent_tokenize(Feature_Data[num])\n",
        "\n",
        "  for sentence in text:\n",
        "    length = nltk.word_tokenize(sentence)\n",
        "    length = len(length)\n",
        "\n",
        "    Average_length2.append(length)\n",
        "\n",
        "  Average_length = sum(list(Average_length2)) \n",
        "\n",
        "  Average_length = Average_length / len(Average_length2)\n",
        "\n",
        "  return Average_length\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Vocab_richness(num): \n",
        "  vectorizer = CountVectorizer(strip_accents=\"unicode\", stop_words=\"english\", min_df=0)\n",
        "  Bow = []\n",
        "  Bow.append(Feature_Data[num])\n",
        "  X = vectorizer.fit_transform(Bow)\n",
        "  Vocab = len(vectorizer.get_feature_names())\n",
        "\n",
        "\n",
        "  return Vocab\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aAqNRkCvezw"
      },
      "source": [
        "def AB_comparisons():\n",
        "  global AB_list\n",
        "  global DocA_words\n",
        "  global DocB_words\n",
        "  global AB_final\n",
        "  global AB_count\n",
        "  AB_list = []\n",
        "  Total_sents = []\n",
        "  AB_final = False\n",
        "  AB_count = 0\n",
        "  Common_trigrams = False\n",
        "  Common_topic = False\n",
        "  word_count = 0\n",
        "  sentJaccard_dict = {}\n",
        "  Trigram_counts = 0\n",
        "\n",
        "  A_trigrams = Complete_list_A[0]\n",
        "  B_trigram1 = Complete_list_B[0][0]\n",
        "  B_trigram2 = Complete_list_B[0][1]\n",
        "  B_trigram3 = Complete_list_B[0][2]\n",
        "  B_trigram4 = Complete_list_B[0][3]\n",
        "\n",
        "  if B_trigram1 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if B_trigram2 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if B_trigram3 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if B_trigram4 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if Trigram_counts > 1:\n",
        "    Common_trigrams = True\n",
        "    AB_list.append(Common_trigrams)\n",
        "    AB_count = AB_count + 1\n",
        "\n",
        "  else:\n",
        "    AB_list.append(Common_trigrams)\n",
        "\n",
        "\n",
        "#Standard deviation:\n",
        "  A1 = np.array(Complete_list_A[1])\n",
        "  B = np.array(Complete_list_B[1])\n",
        "  AB_Dev = A1-B\n",
        "\n",
        "  AB_list.append(list(AB_Dev))\n",
        "\n",
        "\n",
        "#Mean:\n",
        "  A1 = np.array(Complete_list_A[2])\n",
        "  B = np.array(Complete_list_B[2])\n",
        "  AB_Mean = A1-B\n",
        "\n",
        "\n",
        "  AB_list.append(list(AB_Mean))\n",
        "\n",
        "\n",
        "#Lexical Diversity:\n",
        "\n",
        "  A1 = np.array(Complete_list_A[3])\n",
        "  B = np.array(Complete_list_B[3])\n",
        "\n",
        "  AB_lex = A1-B\n",
        "\n",
        "  AB_list.append(AB_lex)\n",
        "\n",
        "\n",
        "\n",
        "#Function_words:\n",
        "  A1 = Complete_list_A[4]\n",
        "  B = Complete_list_B[4]\n",
        "  List = []\n",
        "  FunctionWord_Count = 0\n",
        "\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in B:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = B.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 4:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        FunctionWord_Count = FunctionWord_Count + 1\n",
        "\n",
        "  AB_list.append(FunctionWord_Count)\n",
        "\n",
        "\n",
        "\n",
        "#Punctuation:\n",
        "  A1 = Complete_list_A[5]\n",
        "  B = Complete_list_B[5]\n",
        "  List = []\n",
        "  Punctuation_Count = 0\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in B:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = B.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 2:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        Punctuation_Count = Punctuation_Count + 1\n",
        "\n",
        "  AB_list.append(Punctuation_Count)\n",
        "\n",
        "#Average sentence length:\n",
        "  A1 = np.array(Complete_list_A[6])\n",
        "  B = np.array(Complete_list_B[6])\n",
        "  AB_AvrgSents = A1-B\n",
        "\n",
        "  AB_list.append(AB_AvrgSents)\n",
        "\n",
        "\n",
        "#Number of sents:\n",
        "  A1 = np.array(Complete_list_A[7])\n",
        "  B = np.array(Complete_list_B[7])\n",
        "  AB_Sent_count = A1-B\n",
        "\n",
        "  AB_list.append(AB_Sent_count)\n",
        "\n",
        "#Number of words:\n",
        "  A1 = np.array(Complete_list_A[8])\n",
        "  B = np.array(Complete_list_B[8])\n",
        "  AB_Word_count= A1-B\n",
        "\n",
        "  AB_list.append(AB_Word_count)\n",
        "\n",
        "\n",
        "#TF-IDF score\n",
        "  TFIDF_AB = [] \n",
        "  TFIDF_AB.append(Feature_Data[0])\n",
        "  TFIDF_AB.append(Feature_Data[1])\n",
        "\n",
        "\n",
        "\n",
        "  TF_IDFvectorizer = TfidfVectorizer(strip_accents=\"unicode\", min_df = 0, stop_words='english')\n",
        "\n",
        "  word_tfidf_matrix = TF_IDFvectorizer.fit_transform(TFIDF_AB)\n",
        "\n",
        "  word_cosine_sim = cosine_similarity(word_tfidf_matrix, word_tfidf_matrix)\n",
        "\n",
        "  Docs = [\"A\", \"B\"]\n",
        "\n",
        "\n",
        "  AB_TFIDF = pd.DataFrame(word_cosine_sim,index=sorted(Docs),columns=sorted(Docs))\n",
        "\n",
        "  ABtfidf_Val = AB_TFIDF.iloc[1]['A']\n",
        "\n",
        "  AB_list.append(ABtfidf_Val)\n",
        "\n",
        "\n",
        "  \n",
        "#Jaccard Similarity\n",
        "  a = set(word_tokenize(Feature_Data[0])) \n",
        "  b = set(word_tokenize(Feature_Data[1])) \n",
        "  c = a.intersection(b)\n",
        "  \n",
        "  Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "  AB_list.append(Jaccard)\n",
        "\n",
        "\n",
        "\n",
        "# Keywords\n",
        "  DocA_words = keywords(Feature_Data[0]).split(\"\\n\")\n",
        "  DocB_words = keywords(Feature_Data[1]).split(\"\\n\")\n",
        "\n",
        "  A_keywords = []\n",
        "  B_keywords = []\n",
        "\n",
        "  word_count = 0\n",
        "\n",
        "  for word in DocA_words:\n",
        "      A_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for word in DocB_words:\n",
        "      B_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for each_word in A_keywords:\n",
        "    if each_word in B_keywords:\n",
        "        word_count = word_count + 1\n",
        "\n",
        "\n",
        "\n",
        "#Jaccard suspicious sentences:\n",
        "  DocA_sents = sent_tokenize(Feature_Data[0])\n",
        "  DocB_sents = sent_tokenize(Feature_Data[1])\n",
        "\n",
        "  for original_sentences in DocA_sents:\n",
        "      Total_sents.append(original_sentences)\n",
        "\n",
        "  for sentences in DocB_sents:\n",
        "      Total_sents.append(sentences)\n",
        "\n",
        "  for sentenceA, sentenceB in itertools.combinations(Total_sents, 2):\n",
        "      a = set(word_tokenize(sentenceA)) \n",
        "      b = set(word_tokenize(sentenceB)) \n",
        "      c = a.intersection(b)\n",
        "    \n",
        "      Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "       # print(Jaccard)\n",
        "      if sentenceA != sentenceB and sentenceA not in DocA_sents and sentenceB not in DocB_sents:\n",
        "        if Jaccard > 0.3:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "\n",
        "  AB_list.append(sentJaccard_dict)\n",
        "\n",
        "#Word Mover's Distance:\n",
        "  W2V_data = []\n",
        "\n",
        "  D1 = word_tokenize(Feature_Data[0])\n",
        "  D2 = word_tokenize(Feature_Data[1])\n",
        "\n",
        "\n",
        "  W2V_data.append(D1)\n",
        "  W2V_data.append(D2)\n",
        "\n",
        "\n",
        "\n",
        "  Word2Vecmodel = gensim.models.Word2Vec(W2V_data, min_count = 0,  \n",
        "                                  size = 100, window = 5, sg =1) \n",
        "    \n",
        "  Word2Vecmodel.init_sims(replace=True) \n",
        "\n",
        "  #print(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[1]))\n",
        "\n",
        "  AB_list.append(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[1]))\n",
        "          \n",
        "\n",
        "# Topic\n",
        "  if Complete_list_A[-1] == Complete_list_B[-1]:\n",
        "    Common_topic = True\n",
        "    AB_list.append(Common_topic)\n",
        "\n",
        "  else:\n",
        "    AB_list.append(Common_topic)\n",
        "\n",
        "\n",
        "  \n",
        "  #print(AB_list)\n",
        "  #print(\"topic:\", Complete_list_B[-1])\n",
        "\n",
        "  if AB_list[0] == True:\n",
        "    AB_count = AB_count + 1\n",
        "\n",
        "  if -2.00 <= float(AB_list[1][0]) <= 2:\n",
        "    AB_count = AB_count + 0.5\n",
        "\n",
        "  if -2.00 <= float(AB_list[2][0]) <= 2:\n",
        "    AB_count = AB_count + 0.5\n",
        "\n",
        "  if -2  <= float(AB_list[3]) <= 2:\n",
        "    AB_count = AB_count + 0.5\n",
        "\n",
        "  if Punctuation_Count >= 2:\n",
        "    AB_count = AB_count + 0.5\n",
        "\n",
        "  if FunctionWord_Count >= 2:\n",
        "    AB_count = AB_count + 0.5\n",
        "\n",
        "  if -2 <= AB_list[6] <= 2:\n",
        "    AB_count = AB_count + 0.5\n",
        "  \n",
        "  if -2 <= AB_list[7] <= 2:\n",
        "    AB_count = AB_count + 0.5\n",
        "\n",
        "  if -3 <= AB_list[8] <= 3:\n",
        "    AB_count = AB_count + 0.5\n",
        "\n",
        "  if AB_list[9] > 0.3:\n",
        "    AB_count = AB_count + 1\n",
        "\n",
        "  if AB_list[10] > 0.3:\n",
        "    AB_count = AB_count + 1\n",
        "\n",
        "  if word_count > 2:\n",
        "    AB_count = AB_count + 1\n",
        "\n",
        "  if len(sentJaccard_dict) >= 1:\n",
        "    AB_count = AB_count + 2\n",
        "\n",
        "  if AB_list[12] < 0.1:\n",
        "    AB_count = AB_count + 1\n",
        "\n",
        "\n",
        "  if AB_list[-1] == True:\n",
        "    AB_count = AB_count + 1\n",
        "\n",
        "  \n",
        "  if AB_count > 4:\n",
        "    AB_final = True\n",
        "    Accused[Name_list[1] + str(Name_tags)] = Name_list[0]\n",
        "\n",
        "  #print(\"\\n\", AB_final, AB_count, \"\\n\")\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "def AC_comparisons():\n",
        "  global AC_list\n",
        "  global DocC_words\n",
        "  global AC_final\n",
        "  global AC_count\n",
        "  AC_list = []\n",
        "  AC_count = 0\n",
        "  AC_final = False\n",
        "  Common_trigrams = False\n",
        "  Common_topic = False\n",
        "  word_count = 0\n",
        "  sentJaccard_dict = {}\n",
        "  Total_sents = []\n",
        "\n",
        "  Trigram_counts = 0\n",
        "\n",
        "  A_trigrams = Complete_list_A[0]\n",
        "  C_trigram1 = Complete_list_C[0][0]\n",
        "  C_trigram2 = Complete_list_C[0][1]\n",
        "  C_trigram3 = Complete_list_C[0][2]\n",
        "  C_trigram4 = Complete_list_C[0][3]\n",
        "\n",
        "  if C_trigram1 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if C_trigram2 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if C_trigram3 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if C_trigram4 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if Trigram_counts > 1:\n",
        "    Common_trigrams = True\n",
        "    AC_list.append(Common_trigrams)\n",
        "\n",
        "  else:\n",
        "    AC_list.append(Common_trigrams)\n",
        "    AC_count = AC_count + 1\n",
        "\n",
        "\n",
        "#Standard deviation:\n",
        "  A1 = np.array(Complete_list_A[1])\n",
        "  C = np.array(Complete_list_C[1])\n",
        "  AC_Dev = A1-C\n",
        "\n",
        "  AC_list.append(list(AC_Dev))\n",
        "\n",
        "\n",
        "#Mean:\n",
        "  A1 = np.array(Complete_list_A[2])\n",
        "  C = np.array(Complete_list_C[2])\n",
        "  AC_Mean = A1-C\n",
        "\n",
        "\n",
        "  AC_list.append(list(AC_Mean))\n",
        "\n",
        "\n",
        "#Lexical Diversity:\n",
        "\n",
        "  A1 = np.array(Complete_list_A[3])\n",
        "  C = np.array(Complete_list_C[3])\n",
        "\n",
        "  AC_lex = A1-C\n",
        "\n",
        "  AC_list.append(AC_lex)\n",
        "\n",
        "\n",
        "\n",
        "#Function_words:\n",
        "  A1 = Complete_list_A[4]\n",
        "  C = Complete_list_C[4]\n",
        "  List = []\n",
        "  FunctionWord_Count = 0\n",
        "\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in C:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = C.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 4:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        FunctionWord_Count = FunctionWord_Count + 1\n",
        "\n",
        "  AC_list.append(FunctionWord_Count)\n",
        "\n",
        "\n",
        "\n",
        "#Punctuation:\n",
        "  A1 = Complete_list_A[5]\n",
        "  C = Complete_list_C[5]\n",
        "  List = []\n",
        "  Punctuation_Count = 0\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in C:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = C.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 2:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        Punctuation_Count = Punctuation_Count + 1\n",
        "\n",
        "  AC_list.append(Punctuation_Count)\n",
        "\n",
        "#Average sentence length:\n",
        "  A1 = np.array(Complete_list_A[6])\n",
        "  C = np.array(Complete_list_C[6])\n",
        "  AC_AvrgSents = A1-C\n",
        "\n",
        "  AC_list.append(AC_AvrgSents)\n",
        "\n",
        "\n",
        "#Number of sents:\n",
        "  A1 = np.array(Complete_list_A[7])\n",
        "  C = np.array(Complete_list_C[7])\n",
        "  AC_Sent_count = A1-C\n",
        "\n",
        "  AC_list.append(AC_Sent_count)\n",
        "\n",
        "#Number of words:\n",
        "  A1 = np.array(Complete_list_A[8])\n",
        "  C = np.array(Complete_list_C[8])\n",
        "  AC_Word_count= A1-C\n",
        "\n",
        "  AC_list.append(AC_Word_count)\n",
        "\n",
        "\n",
        "\n",
        "#TF-IDF score\n",
        "  TFIDF_AC = [] \n",
        "  TFIDF_AC.append(Feature_Data[0])\n",
        "  TFIDF_AC.append(Feature_Data[2])\n",
        "\n",
        "\n",
        "\n",
        "  TF_IDFvectorizer = TfidfVectorizer(strip_accents=\"unicode\", min_df = 0, stop_words='english')\n",
        "\n",
        "  word_tfidf_matrix = TF_IDFvectorizer.fit_transform(TFIDF_AC)\n",
        "\n",
        "  word_cosine_sim = cosine_similarity(word_tfidf_matrix, word_tfidf_matrix)\n",
        "\n",
        "  Docs = [\"A\", \"C\"]\n",
        "\n",
        "  AC_TFIDF = pd.DataFrame(word_cosine_sim,index=sorted(Docs),columns=sorted(Docs))\n",
        "\n",
        "  ACtfidf_Val = AC_TFIDF.iloc[1]['A']\n",
        "\n",
        "  AC_list.append(ACtfidf_Val)\n",
        "\n",
        "\n",
        "  \n",
        "#Jaccard Similarity\n",
        "  a = set(word_tokenize(Feature_Data[0])) \n",
        "  b = set(word_tokenize(Feature_Data[2])) \n",
        "  c = a.intersection(b)\n",
        "  \n",
        "  Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "  AC_list.append(Jaccard)\n",
        "\n",
        "\n",
        "# Keywords\n",
        "  DocA_words = keywords(Feature_Data[0]).split(\"\\n\")\n",
        "  DocC_words = keywords(Feature_Data[2]).split(\"\\n\")\n",
        "\n",
        "  A_keywords = []\n",
        "  B_keywords = []\n",
        "\n",
        "  word_count = 0\n",
        "\n",
        "  for word in DocA_words:\n",
        "      A_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for word in DocC_words:\n",
        "      B_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for each_word in A_keywords:\n",
        "    if each_word in B_keywords:\n",
        "        word_count = word_count + 1\n",
        "\n",
        "\n",
        "\n",
        "#Jaccard suspicious sentences:\n",
        "  DocA_sents = sent_tokenize(Feature_Data[0])\n",
        "  DocC_sents = sent_tokenize(Feature_Data[2])\n",
        "\n",
        "  for original_sentences in DocA_sents:\n",
        "      Total_sents.append(original_sentences)\n",
        "\n",
        "  for sentences in DocC_sents:\n",
        "      Total_sents.append(sentences)\n",
        "\n",
        "  for sentenceA, sentenceB in itertools.combinations(Total_sents, 2):\n",
        "      a = set(word_tokenize(sentenceA)) \n",
        "      b = set(word_tokenize(sentenceB)) \n",
        "      c = a.intersection(b)\n",
        "    \n",
        "      Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "       # print(Jaccard)\n",
        "\n",
        "      if sentenceA != sentenceB and sentenceA not in DocA_sents and sentenceB not in DocC_sents:\n",
        "        if Jaccard > 0.3:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "\n",
        "  AC_list.append(sentJaccard_dict)\n",
        "\n",
        "#Word Mover's Distance:\n",
        "  W2V_data = []\n",
        "\n",
        "  D1 = word_tokenize(Feature_Data[0])\n",
        "  D2 = word_tokenize(Feature_Data[2])\n",
        "\n",
        "\n",
        "  W2V_data.append(D1)\n",
        "  W2V_data.append(D2)\n",
        "\n",
        "\n",
        "\n",
        "  Word2Vecmodel = gensim.models.Word2Vec(W2V_data, min_count = 0,  \n",
        "                                  size = 100, window = 5, sg =1) \n",
        "    \n",
        "  Word2Vecmodel.init_sims(replace=True) \n",
        "\n",
        "  #print(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[2]))\n",
        "  \n",
        "  AC_list.append(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[2]))\n",
        "\n",
        "\n",
        "#Topics:\n",
        "\n",
        "  if Complete_list_A[-1] == Complete_list_C[-1]:\n",
        "    Common_topic = True\n",
        "    AC_list.append(Common_topic)\n",
        "\n",
        "  else:\n",
        "    AC_list.append(Common_topic)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        " # print(AC_list)\n",
        "  #print(\"topic:\", Complete_list_C[-1])\n",
        "\n",
        "\n",
        "  if AC_list[0] == True:\n",
        "    AC_count = AC_count + 1\n",
        "\n",
        "  if -2 <= float(AC_list[1][0]) <= 2:\n",
        "    AC_count = AC_count + 0.5\n",
        "\n",
        "  if -2 <= float(AC_list[2][0]) <= 2:\n",
        "    AC_count = AC_count + 0.5\n",
        "\n",
        "  if -2 <= float(AC_list[3]) <= 2:\n",
        "    AC_count = AC_count + 0.5\n",
        "\n",
        "  if Punctuation_Count >= 2:\n",
        "    AC_count = AC_count + 0.5\n",
        "\n",
        "  if FunctionWord_Count >= 2:\n",
        "    AC_count = AC_count + 0.5\n",
        "  \n",
        "  if -2 <= AC_list[6] <= 2:\n",
        "    AC_count = AC_count + 0.5\n",
        "  \n",
        "  if -2 <= AC_list[7] <= 2:\n",
        "    AC_count = AC_count + 0.5\n",
        "\n",
        "  if -3 <= AC_list[8] <= 3:\n",
        "    AC_count = AC_count + 0.5\n",
        "\n",
        "  if AC_list[9] > 0.3:\n",
        "    AC_count = AC_count + 1\n",
        "\n",
        "  if AC_list[10] > 0.3:\n",
        "    AC_count = AC_count + 1\n",
        "\n",
        "  if word_count >= 2:\n",
        "    AC_count = AC_count + 1\n",
        "\n",
        "  if len(sentJaccard_dict) >= 1:\n",
        "    AC_count = AC_count + 1\n",
        "\n",
        "  if AC_list[12] < 0.1:\n",
        "    AC_count = AC_count + 1\n",
        "\n",
        "  if AC_list[-1] == True:\n",
        "    AC_count = AC_count + 1\n",
        "\n",
        "  \n",
        "  if AC_count > 4:\n",
        "    AC_final = True\n",
        "    Accused[Name_list[2] + str(Name_tags)] = Name_list[0]\n",
        "\n",
        " # print(\"\\n\", AC_final, AC_count, \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def AD_comparisons():\n",
        "  global AD_list\n",
        "  global DocD_words\n",
        "  global AD_final\n",
        "  global AD_count\n",
        "  AD_list = []\n",
        "  Total_sents = []\n",
        "  AD_final = False\n",
        "  AD_count = 0\n",
        "  Common_trigrams = False\n",
        "  Common_topic = False\n",
        "  word_count = 0\n",
        "  sentJaccard_dict = {}\n",
        "\n",
        "  Trigram_counts = 0\n",
        "\n",
        "  A_trigrams = Complete_list_A[0]\n",
        "  D_trigram1 = Complete_list_D[0][0]\n",
        "  D_trigram2 = Complete_list_D[0][1]\n",
        "  D_trigram3 = Complete_list_D[0][2]\n",
        "  D_trigram4 = Complete_list_D[0][3]\n",
        "\n",
        "  if D_trigram1 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if D_trigram2 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if D_trigram3 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if D_trigram4 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if Trigram_counts > 1:\n",
        "    Common_trigrams = True\n",
        "    AD_list.append(Common_trigrams)\n",
        "\n",
        "  else:\n",
        "    AD_list.append(Common_trigrams)\n",
        "    AD_count = AD_count + 1\n",
        "\n",
        "\n",
        "#Standard deviation:\n",
        "  A1 = np.array(Complete_list_A[1])\n",
        "  D = np.array(Complete_list_D[1])\n",
        "  AD_Dev = A1-D\n",
        "\n",
        "  AD_list.append(list(AD_Dev))\n",
        "\n",
        "\n",
        "#Mean:\n",
        "  A1 = np.array(Complete_list_A[2])\n",
        "  D = np.array(Complete_list_D[2])\n",
        "  AD_Mean = A1-D\n",
        "\n",
        "\n",
        "  AD_list.append(list(AD_Mean))\n",
        "\n",
        "\n",
        "#Lexical Diversity:\n",
        "\n",
        "  A1 = np.array(Complete_list_A[3])\n",
        "  D = np.array(Complete_list_D[3])\n",
        "\n",
        "  AD_lex = A1-D\n",
        "\n",
        "  AD_list.append(AD_lex)\n",
        "\n",
        "\n",
        "\n",
        "#Function_words:\n",
        "  A1 = Complete_list_A[4]\n",
        "  D = Complete_list_D[4]\n",
        "  List = []\n",
        "  FunctionWord_Count = 0\n",
        "\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in D:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = D.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 4:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        FunctionWord_Count = FunctionWord_Count + 1\n",
        "\n",
        "  AD_list.append(FunctionWord_Count)\n",
        "\n",
        "\n",
        "\n",
        "#Punctuation:\n",
        "  A1 = Complete_list_A[5]\n",
        "  D = Complete_list_D[5]\n",
        "  List = []\n",
        "  Punctuation_Count = 0\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in D:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = D.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 2:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        Punctuation_Count = Punctuation_Count + 1\n",
        "\n",
        "  AD_list.append(Punctuation_Count)\n",
        "\n",
        "\n",
        "#Average sentence length:\n",
        "  A1 = np.array(Complete_list_A[6])\n",
        "  D = np.array(Complete_list_D[6])\n",
        "  AD_AvrgSents = A1-D\n",
        "\n",
        "  AD_list.append(AD_AvrgSents)\n",
        "\n",
        "\n",
        "#Number of sents:\n",
        "  A1 = np.array(Complete_list_A[7])\n",
        "  D = np.array(Complete_list_D[7])\n",
        "  AD_Sent_count = A1-D\n",
        "\n",
        "  AD_list.append(AD_Sent_count)\n",
        "\n",
        "#Number of words:\n",
        "  A1 = np.array(Complete_list_A[8])\n",
        "  D = np.array(Complete_list_D[8])\n",
        "  AD_Word_count= A1-D\n",
        "\n",
        "  AD_list.append(AD_Word_count)\n",
        "\n",
        "\n",
        "#TF-IDF score\n",
        "  TFIDF_AD = [] \n",
        "  TFIDF_AD.append(Feature_Data[0])\n",
        "  TFIDF_AD.append(Feature_Data[3])\n",
        "\n",
        "\n",
        "\n",
        "  TF_IDFvectorizer = TfidfVectorizer(strip_accents=\"unicode\", min_df = 0, stop_words='english')\n",
        "\n",
        "  word_tfidf_matrix = TF_IDFvectorizer.fit_transform(TFIDF_AD)\n",
        "\n",
        "  word_cosine_sim = cosine_similarity(word_tfidf_matrix, word_tfidf_matrix)\n",
        "\n",
        "  Docs = [\"A\", \"D\"]\n",
        "\n",
        "  AD_TFIDF = pd.DataFrame(word_cosine_sim,index=sorted(Docs),columns=sorted(Docs))\n",
        "\n",
        "  ADtfidf_Val = AD_TFIDF.iloc[1]['A']\n",
        "\n",
        "  AD_list.append(ADtfidf_Val)\n",
        "\n",
        "\n",
        "  \n",
        "#Jaccard Similarity\n",
        "  a = set(word_tokenize(Feature_Data[0])) \n",
        "  b = set(word_tokenize(Feature_Data[3])) \n",
        "  c = a.intersection(b)\n",
        "  \n",
        "  Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "  AD_list.append(Jaccard)\n",
        "\n",
        "\n",
        "# Keywords\n",
        "  DocA_words = keywords(Feature_Data[0]).split(\"\\n\")\n",
        "  DocD_words = keywords(Feature_Data[3]).split(\"\\n\")\n",
        "\n",
        "  A_keywords = []\n",
        "  B_keywords = []\n",
        "\n",
        "  word_count = 0\n",
        "\n",
        "  for word in DocA_words:\n",
        "      A_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for word in DocD_words:\n",
        "      B_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for each_word in A_keywords:\n",
        "    if each_word in B_keywords:\n",
        "        word_count = word_count + 1\n",
        "\n",
        "\n",
        "\n",
        "#Jaccard suspicious sentences:\n",
        "  DocA_sents = sent_tokenize(Feature_Data[0])\n",
        "  DocD_sents = sent_tokenize(Feature_Data[3])\n",
        "\n",
        "  for original_sentences in DocA_sents:\n",
        "      Total_sents.append(original_sentences)\n",
        "\n",
        "  for sentences in DocD_sents:\n",
        "      Total_sents.append(sentences)\n",
        "\n",
        "  for sentenceA, sentenceB in itertools.combinations(Total_sents, 2):\n",
        "      a = set(word_tokenize(sentenceA)) \n",
        "      b = set(word_tokenize(sentenceB)) \n",
        "      c = a.intersection(b)\n",
        "    \n",
        "      Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "       # print(Jaccard)\n",
        "\n",
        "      if sentenceA != sentenceB and sentenceA not in DocA_sents and sentenceB not in DocD_sents:\n",
        "        if Jaccard > 0.3:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "\n",
        "  AD_list.append(sentJaccard_dict)\n",
        "\n",
        "#Word Mover's Distance:\n",
        "  W2V_data = []\n",
        "\n",
        "  D1 = word_tokenize(Feature_Data[0])\n",
        "  D2 = word_tokenize(Feature_Data[3])\n",
        "\n",
        "\n",
        "  W2V_data.append(D1)\n",
        "  W2V_data.append(D2)\n",
        "\n",
        "\n",
        "\n",
        "  Word2Vecmodel = gensim.models.Word2Vec(W2V_data, min_count = 0,  \n",
        "                                  size = 100, window = 5, sg =1) \n",
        "    \n",
        "  Word2Vecmodel.init_sims(replace=True) \n",
        "\n",
        "  #print(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[3]))\n",
        "  \n",
        "  AD_list.append(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[3]))\n",
        "\n",
        "\n",
        "#Topics:\n",
        "\n",
        "  if Complete_list_A[-1] == Complete_list_D[-1]:\n",
        "    Common_topic = True\n",
        "    AD_list.append(Common_topic)\n",
        "\n",
        "  else:\n",
        "    AD_list.append(Common_topic)\n",
        "\n",
        "\n",
        " # print(AD_list)\n",
        "\n",
        " # print(\"topic:\", Complete_list_D[-1])\n",
        "\n",
        "  if AD_list[0] == True:\n",
        "    AD_count = AD_count + 1\n",
        "\n",
        "  if -2 <= float(AD_list[1][0]) <= 2:\n",
        "    AD_count = AD_count + 0.5\n",
        "\n",
        "  if -2 <= float(AD_list[2][0]) <= 2:\n",
        "    AD_count = AD_count + 0.5\n",
        "\n",
        "  if -2 <= AD_list[3] <= 2:\n",
        "    AD_count = AD_count + 0.5\n",
        "\n",
        "  if Punctuation_Count >= 2:\n",
        "    AD_count = AD_count + 0.5\n",
        "\n",
        "  if FunctionWord_Count >= 2:\n",
        "    AD_count = AD_count + 0.5\n",
        "\n",
        "  if -2 <= AD_list[6] <= 2:\n",
        "    AD_count = AD_count + 0.5\n",
        "  \n",
        "  if -3 <= AD_list[7] <= 3:\n",
        "    AD_count = AD_count + 0.5\n",
        "\n",
        "  if -2 <= AD_list[8] <= 2:\n",
        "    AD_count = AD_count + 0.5\n",
        "\n",
        "  if AD_list[9] > 0.3:\n",
        "    AD_count = AD_count + 1\n",
        "\n",
        "  if AD_list[10] > 0.3:\n",
        "    AD_count = AD_count + 1\n",
        "\n",
        "  if word_count >= 2:\n",
        "    AD_count = AD_count + 1\n",
        "\n",
        "  if len(sentJaccard_dict) >= 1:\n",
        "    AD_count = AD_count + 1\n",
        "\n",
        "  if AD_list[12] < 0.1:\n",
        "    AD_count = AD_count + 1\n",
        "\n",
        "  if AD_list[-1] == True:\n",
        "    AD_count = AD_count + 1\n",
        "\n",
        "  \n",
        "  if AD_count > 4:\n",
        "    AD_final = True\n",
        "    Accused[Name_list[3] + str(Name_tags)] = Name_list[0]\n",
        "\n",
        " # print(\"\\n\", AD_final, AD_count, \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def AE_comparisons():\n",
        "  global AE_list\n",
        "  global DocE_words\n",
        "  global AE_final\n",
        "  global AE_count\n",
        "  AE_list = []\n",
        "  Total_sents = []\n",
        "  sentJaccard_dict = {}\n",
        "  word_count = 0\n",
        "  AE_final = False\n",
        "  AE_count = 0\n",
        "  Common_trigrams = False\n",
        "  Common_topic = False\n",
        "\n",
        "  Trigram_counts = 0\n",
        "\n",
        "  A_trigrams = Complete_list_A[0]\n",
        "  E_trigram1 = Complete_list_E[0][0]\n",
        "  E_trigram2 = Complete_list_E[0][1]\n",
        "  E_trigram3 = Complete_list_E[0][2]\n",
        "  E_trigram4 = Complete_list_E[0][3]\n",
        "\n",
        "  if E_trigram1 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if E_trigram2 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if E_trigram3 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if E_trigram4 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  \n",
        "\n",
        "  if Trigram_counts > 1:\n",
        "    Common_trigrams = True\n",
        "    AE_list.append(Common_trigrams)\n",
        "\n",
        "  else:\n",
        "    AE_list.append(Common_trigrams)\n",
        "    AE_count = AE_count + 1\n",
        "\n",
        "\n",
        "#Standard deviation:\n",
        "  A1 = np.array(Complete_list_A[1])\n",
        "  E = np.array(Complete_list_E[1])\n",
        "  AE_Dev = A1-E\n",
        "\n",
        "  AE_list.append(list(AE_Dev))\n",
        "\n",
        "\n",
        "#Mean:\n",
        "  A1 = np.array(Complete_list_A[2])\n",
        "  E = np.array(Complete_list_E[2])\n",
        "  AE_Mean = A1-E\n",
        "\n",
        "\n",
        "  AE_list.append(list(AE_Mean))\n",
        "\n",
        "\n",
        "#Lexical Diversity:\n",
        "\n",
        "  A1 = np.array(Complete_list_A[3])\n",
        "  E = np.array(Complete_list_E[3])\n",
        "\n",
        "  AE_lex = A1-E\n",
        "\n",
        "  AE_list.append(AE_lex)\n",
        "\n",
        "\n",
        "\n",
        "#Function_words:\n",
        "  A1 = Complete_list_A[4]\n",
        "  E = Complete_list_E[4]\n",
        "  List = []\n",
        "  FunctionWord_Count = 0\n",
        "\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in E:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = E.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 4:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        FunctionWord_Count = FunctionWord_Count + 1\n",
        "\n",
        "  AE_list.append(FunctionWord_Count)\n",
        "\n",
        "\n",
        "\n",
        "#Punctuation:\n",
        "  A1 = Complete_list_A[5]\n",
        "  E = Complete_list_E[5]\n",
        "  List = []\n",
        "  Punctuation_Count = 0\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in E:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = E.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 2:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        Punctuation_Count = Punctuation_Count + 1\n",
        "\n",
        "  AE_list.append(Punctuation_Count)\n",
        "\n",
        "\n",
        "#Average sentence length:\n",
        "  A1 = np.array(Complete_list_E[6])\n",
        "  E = np.array(Complete_list_E[6])\n",
        "  AE_AvrgSents = A1-E\n",
        "\n",
        "  AE_list.append(AE_AvrgSents)\n",
        "\n",
        "\n",
        "#Number of sents:\n",
        "  A1 = np.array(Complete_list_A[7])\n",
        "  E = np.array(Complete_list_E[7])\n",
        "  AE_Sent_count = A1-E\n",
        "\n",
        "  AE_list.append(AE_Sent_count)\n",
        "\n",
        "#Number of words:\n",
        "  A1 = np.array(Complete_list_A[8])\n",
        "  E = np.array(Complete_list_E[8])\n",
        "  AE_Word_count= A1-E\n",
        "\n",
        "  AE_list.append(AE_Word_count)\n",
        "\n",
        "\n",
        "#TF-IDF score\n",
        "  TFIDF_AE = [] \n",
        "  TFIDF_AE.append(Feature_Data[0])\n",
        "  TFIDF_AE.append(Feature_Data[4])\n",
        "\n",
        "\n",
        "\n",
        "  TF_IDFvectorizer = TfidfVectorizer(strip_accents=\"unicode\", min_df = 0, stop_words='english')\n",
        "\n",
        "  word_tfidf_matrix = TF_IDFvectorizer.fit_transform(TFIDF_AE)\n",
        "\n",
        "  word_cosine_sim = cosine_similarity(word_tfidf_matrix, word_tfidf_matrix)\n",
        "\n",
        "  Docs = [\"A\", \"E\"]\n",
        "\n",
        "  AE_TFIDF = pd.DataFrame(word_cosine_sim,index=sorted(Docs),columns=sorted(Docs))\n",
        "\n",
        "  AEtfidf_Val = AE_TFIDF.iloc[1]['A']\n",
        "\n",
        "  AE_list.append(AEtfidf_Val)\n",
        "\n",
        "\n",
        "  \n",
        "#Jaccard Similarity\n",
        "  a = set(word_tokenize(Feature_Data[0])) \n",
        "  b = set(word_tokenize(Feature_Data[4])) \n",
        "  c = a.intersection(b)\n",
        "  \n",
        "  Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "  AE_list.append(Jaccard)\n",
        "\n",
        "\n",
        "# Keywords\n",
        "  DocA_words = keywords(Feature_Data[0]).split(\"\\n\")\n",
        "  DocE_words = keywords(Feature_Data[4]).split(\"\\n\")\n",
        "\n",
        "  A_keywords = []\n",
        "  B_keywords = []\n",
        "\n",
        "  word_count = 0\n",
        "\n",
        "  for word in DocA_words:\n",
        "      A_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for word in DocE_words:\n",
        "      B_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for each_word in A_keywords:\n",
        "    if each_word in B_keywords:\n",
        "        word_count = word_count + 1\n",
        "\n",
        "\n",
        "\n",
        "#Jaccard suspicious sentences:\n",
        "  DocA_sents = sent_tokenize(Feature_Data[0])\n",
        "  DocE_sents = sent_tokenize(Feature_Data[4])\n",
        "\n",
        "  for original_sentences in DocA_sents:\n",
        "      Total_sents.append(original_sentences)\n",
        "\n",
        "  for sentences in DocE_sents:\n",
        "      Total_sents.append(sentences)\n",
        "\n",
        "  for sentenceA, sentenceB in itertools.combinations(Total_sents, 2):\n",
        "      a = set(word_tokenize(sentenceA)) \n",
        "      b = set(word_tokenize(sentenceB)) \n",
        "      c = a.intersection(b)\n",
        "    \n",
        "      Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "       # print(Jaccard)\n",
        "\n",
        "      if sentenceA != sentenceB and sentenceA not in DocA_sents and sentenceB not in DocE_sents:\n",
        "        if Jaccard > 0.3:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "\n",
        "  AE_list.append(sentJaccard_dict)\n",
        "\n",
        "#Word Mover's Distance:\n",
        "  W2V_data = []\n",
        "\n",
        "  D1 = word_tokenize(Feature_Data[0])\n",
        "  D2 = word_tokenize(Feature_Data[4])\n",
        "\n",
        "\n",
        "  W2V_data.append(D1)\n",
        "  W2V_data.append(D2)\n",
        "\n",
        "\n",
        "\n",
        "  Word2Vecmodel = gensim.models.Word2Vec(W2V_data, min_count = 0,  \n",
        "                                  size = 100, window = 5, sg =1) \n",
        "    \n",
        "  Word2Vecmodel.init_sims(replace=True) \n",
        "\n",
        "  #print(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[4]))\n",
        "  \n",
        "  AE_list.append(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[4]))\n",
        "\n",
        "\n",
        "\n",
        "#Topics:\n",
        "\n",
        "  if Complete_list_A[-1] == Complete_list_E[-1]:\n",
        "    Common_topic = True\n",
        "    AE_list.append(Common_topic)\n",
        "\n",
        "  else:\n",
        "    AE_list.append(Common_topic)\n",
        "\n",
        "\n",
        "  print(AE_list)\n",
        "\n",
        "  if AE_list[0] == True:\n",
        "    AE_count = AE_count + 1\n",
        "\n",
        "  if -2 <= float(AE_list[1][0]) <= 2:\n",
        "    AE_count = AE_count + 0.5\n",
        "\n",
        "  if -2 <= float(AE_list[2][0]) <= 2:\n",
        "    AE_count = AE_count + 0.5\n",
        "\n",
        "  if -2 <= AE_list[3] <= 2:\n",
        "    AE_count = AE_count + 0.5\n",
        "\n",
        "  if Punctuation_Count >= 2:\n",
        "    AE_count = AE_count + 0.5\n",
        "\n",
        "  if FunctionWord_Count >= 2:\n",
        "    AE_count = AE_count + 0.5\n",
        "\n",
        "  if -2 <= AE_list[6] <= 2:\n",
        "    AE_count = AE_count + 0.5\n",
        "  \n",
        "  if -2 <= AE_list[7] <= 2:\n",
        "    AE_count = AE_count + 0.5\n",
        "\n",
        "  if -3 <= AE_list[8] <= 3:\n",
        "    AE_count = AE_count + 0.5\n",
        "\n",
        "  if AE_list[9] > 0.3:\n",
        "    AE_count = AE_count + 1\n",
        "\n",
        "  if AE_list[10] > 0.3:\n",
        "    AE_count = AE_count + 1\n",
        "\n",
        "  if word_count >= 1:\n",
        "    AE_count = AE_count + 1\n",
        "\n",
        "  if len(sentJaccard_dict) >= 1:\n",
        "    AE_count = AE_count + 1\n",
        "\n",
        "  if AE_list[12] < 0.1:\n",
        "    AE_count = AE_count + 1\n",
        "\n",
        "  if AE_list[-1] == True:\n",
        "    AE_count = AE_count + 1\n",
        "\n",
        "  \n",
        "  if AE_count > 4:\n",
        "    AE_final = True\n",
        "    Accused[Name_list[4] + str(Name_tags)] = Name_list[0]\n",
        "\n",
        " # print(\"topic:\", Complete_list_E[-1])\n",
        " # print(\"\\n\", AE_final, AE_count, \"\\n\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tCMcIgVj0to"
      },
      "source": [
        "# Define All Comparison Functions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2swlu5y-jyS-"
      },
      "source": [
        "def AB_comparisons():\n",
        "  global AB_list\n",
        "  global DocA_words\n",
        "  global DocB_words\n",
        "  global AB_final\n",
        "  global AB_count\n",
        "  AB_list = []\n",
        "  AB_final = False\n",
        "  AB_count = 0\n",
        "  Common_trigrams = False\n",
        "  Common_topic = False\n",
        "  word_count = 0\n",
        "  Trigram_counts = 0\n",
        "\n",
        "  A_trigrams = Complete_list_A[0]\n",
        "  B_trigram1 = Complete_list_B[0][0]\n",
        "  B_trigram2 = Complete_list_B[0][1]\n",
        "  B_trigram3 = Complete_list_B[0][2]\n",
        "  B_trigram4 = Complete_list_B[0][3]\n",
        "\n",
        "  if B_trigram1 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if B_trigram2 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if B_trigram3 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if B_trigram4 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if Trigram_counts > 1:\n",
        "    Common_trigrams = True\n",
        "    AB_list.append(Common_trigrams)\n",
        "    AB_count = AB_count + 1\n",
        "\n",
        "  else:\n",
        "    AB_list.append(Common_trigrams)\n",
        "\n",
        "\n",
        "#Standard deviation:\n",
        "  A1 = np.array(Complete_list_A[1])\n",
        "  B = np.array(Complete_list_B[1])\n",
        "  AB_Dev = A1-B\n",
        "\n",
        "  AB_list.append(list(AB_Dev))\n",
        "\n",
        "\n",
        "#Mean:\n",
        "  A1 = np.array(Complete_list_A[2])\n",
        "  B = np.array(Complete_list_B[2])\n",
        "  AB_Mean = A1-B\n",
        "\n",
        "\n",
        "  AB_list.append(list(AB_Mean))\n",
        "\n",
        "\n",
        "#Lexical Diversity:\n",
        "\n",
        "  A1 = np.array(Complete_list_A[3])\n",
        "  B = np.array(Complete_list_B[3])\n",
        "\n",
        "  AB_lex = A1-B\n",
        "\n",
        "  AB_list.append(AB_lex)\n",
        "\n",
        "\n",
        "\n",
        "#Function_words:\n",
        "  A1 = Complete_list_A[4]\n",
        "  B = Complete_list_B[4]\n",
        "  List = []\n",
        "  FunctionWord_Count = 0\n",
        "\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in B:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = B.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 3:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        FunctionWord_Count = FunctionWord_Count + 1\n",
        "\n",
        "  AB_list.append(FunctionWord_Count)\n",
        "\n",
        "\n",
        "\n",
        "#Punctuation:\n",
        "  A1 = Complete_list_A[5]\n",
        "  B = Complete_list_B[5]\n",
        "  List = []\n",
        "  Punctuation_Count = 0\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in B:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = B.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 2:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        Punctuation_Count = Punctuation_Count + 1\n",
        "\n",
        "  AB_list.append(Punctuation_Count)\n",
        "\n",
        "#Average sentence length:\n",
        "  A1 = np.array(Complete_list_A[6])\n",
        "  B = np.array(Complete_list_B[6])\n",
        "  AB_AvrgSents = A1-B\n",
        "\n",
        "  AB_list.append(AB_AvrgSents)\n",
        "\n",
        "\n",
        "#Number of sents:\n",
        "  A1 = np.array(Complete_list_A[7])\n",
        "  B = np.array(Complete_list_B[7])\n",
        "  AB_Sent_count = A1-B\n",
        "\n",
        "  AB_list.append(AB_Sent_count)\n",
        "\n",
        "#Number of words:\n",
        "  A1 = np.array(Complete_list_A[8])\n",
        "  B = np.array(Complete_list_B[8])\n",
        "  AB_Word_count= A1-B\n",
        "\n",
        "  AB_list.append(AB_Word_count)\n",
        "\n",
        "\n",
        "#TF-IDF score\n",
        "  TFIDF_AB = [] \n",
        "  TFIDF_AB.append(Feature_Data[0])\n",
        "  TFIDF_AB.append(Feature_Data[1])\n",
        "\n",
        "\n",
        "\n",
        "  TF_IDFvectorizer = TfidfVectorizer(strip_accents=\"unicode\", min_df = 0, stop_words='english')\n",
        "\n",
        "  word_tfidf_matrix = TF_IDFvectorizer.fit_transform(TFIDF_AB)\n",
        "\n",
        "  word_cosine_sim = cosine_similarity(word_tfidf_matrix, word_tfidf_matrix)\n",
        "\n",
        "  Docs = [\"A\", \"B\"]\n",
        "\n",
        "\n",
        "  AB_TFIDF = pd.DataFrame(word_cosine_sim,index=sorted(Docs),columns=sorted(Docs))\n",
        "\n",
        "  ABtfidf_Val = AB_TFIDF.iloc[1]['A']\n",
        "\n",
        "  AB_list.append(ABtfidf_Val)\n",
        "\n",
        "\n",
        "  \n",
        "#Jaccard Similarity\n",
        "  a = set(word_tokenize(Feature_Data[0])) \n",
        "  b = set(word_tokenize(Feature_Data[1])) \n",
        "  c = a.intersection(b)\n",
        "  \n",
        "  Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "  AB_list.append(Jaccard)\n",
        "\n",
        "\n",
        "\n",
        "# Keywords\n",
        "  DocA_words = keywords(Feature_Data[0], words=5, lemmatize=True).split(\"\\n\")\n",
        "  DocB_words = keywords(Feature_Data[1], words=5, lemmatize=True).split(\"\\n\")\n",
        "\n",
        "  A_keywords = []\n",
        "  B_keywords = []\n",
        "\n",
        "  word_count = 0\n",
        "\n",
        "  for word in DocA_words:\n",
        "      A_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for word in DocB_words:\n",
        "      B_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for each_word in A_keywords:\n",
        "    if each_word in B_keywords:\n",
        "        word_count = word_count + 1\n",
        "\n",
        "\n",
        "\n",
        "#Jaccard suspicious sentences:\n",
        "  DocA_sents = sent_tokenize(Feature_Data[0])\n",
        "  DocB_sents = sent_tokenize(Feature_Data[1])\n",
        "  sentJaccard_dict = {}\n",
        "  Total_sents = []\n",
        "\n",
        "  for original_sentences in DocA_sents:\n",
        "      Total_sents.append(original_sentences)\n",
        "\n",
        "  for sentences in DocB_sents:\n",
        "      Total_sents.append(sentences)\n",
        "\n",
        "  for sentenceA, sentenceB in itertools.combinations(Total_sents, 2):\n",
        "      a = set(word_tokenize(sentenceA)) \n",
        "      b = set(word_tokenize(sentenceB)) \n",
        "      c = a.intersection(b)\n",
        "    \n",
        "      Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "       # print(Jaccard)\n",
        "      if sentenceA != sentenceB and sentenceA not in DocB_sents and sentenceB not in DocA_sents:\n",
        "        if Jaccard > 0.4:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "      elif sentenceA and sentenceB in DocA_sents and DocB_sents:\n",
        "        if Jaccard > 0.4:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "\n",
        "  AB_list.append(sentJaccard_dict)\n",
        "\n",
        "#Word Mover's Distance:\n",
        "  W2V_data = []\n",
        "\n",
        "  D1 = word_tokenize(Feature_Data[0])\n",
        "  D2 = word_tokenize(Feature_Data[1])\n",
        "\n",
        "\n",
        "  W2V_data.append(D1)\n",
        "  W2V_data.append(D2)\n",
        "\n",
        "\n",
        "\n",
        "  Word2Vecmodel = gensim.models.Word2Vec(W2V_data, min_count = 0,  \n",
        "                                  size = 100, window = 5, sg =1) \n",
        "    \n",
        "  Word2Vecmodel.init_sims(replace=True) \n",
        "\n",
        "  #print(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[1]))\n",
        "\n",
        "  AB_list.append(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[1]))\n",
        "          \n",
        "\n",
        "# Topic\n",
        "  if Complete_list_A[-1] == Complete_list_B[-1]:\n",
        "    Common_topic = True\n",
        "    AB_list.append(Common_topic)\n",
        "\n",
        "  else:\n",
        "    AB_list.append(Common_topic)\n",
        "\n",
        "\n",
        "  \n",
        "  #print(AB_list)\n",
        "  #print(\"topic:\", Complete_list_B[-1])\n",
        "\n",
        "  if AB_list[0] == True:\n",
        "    AB_count = AB_count + 0.25\n",
        "\n",
        "  if -2.00 <= float(AB_list[1][0]) <= 2:\n",
        "    AB_count = AB_count + 0.25\n",
        "\n",
        "  if -2.00 <= float(AB_list[2][0]) <= 2:\n",
        "    AB_count = AB_count + 0.25\n",
        "\n",
        "  if -20  <= float(AB_list[3]) <= 20:\n",
        "    AB_count = AB_count + 0.25\n",
        "\n",
        "  if Punctuation_Count >= 2:\n",
        "    AB_count = AB_count + 0.25\n",
        "\n",
        "  if FunctionWord_Count >= 2:\n",
        "    AB_count = AB_count + 0.25\n",
        "\n",
        "  if -2 <= AB_list[6] <= 2:\n",
        "    AB_count = AB_count + 0.25\n",
        "  \n",
        "  if -2 <= AB_list[7] <= 2:\n",
        "    AB_count = AB_count + 0.25\n",
        "\n",
        "  if -20 <= AB_list[8] <= 20:\n",
        "    AB_count = AB_count + 0.25\n",
        "\n",
        "  if AB_list[9] > 0.3:\n",
        "    AB_count = AB_count + 1.5\n",
        "\n",
        "  if AB_list[10] > 0.3:\n",
        "    AB_count = AB_count + 1.5\n",
        "\n",
        "  if word_count > 2:\n",
        "    AB_count = AB_count + 1\n",
        "\n",
        "  if len(sentJaccard_dict) >= 1:\n",
        "    AB_count = AB_count + 1.5\n",
        "\n",
        "  if AB_list[12] < 0.1:\n",
        "    AB_count = AB_count + 1\n",
        "\n",
        "\n",
        "  if AB_list[-1] == True:\n",
        "    AB_count = AB_count + 0.5\n",
        "\n",
        "  \n",
        "  if AB_count > 5:\n",
        "    AB_final = True\n",
        "    Accused[Name_list[1] + str(Name_tags)] = Name_list[0]\n",
        "\n",
        "  #print(\"\\n\", AB_final, AB_count, \"\\n\")\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "def AC_comparisons():\n",
        "  global AC_list\n",
        "  global DocC_words\n",
        "  global AC_final\n",
        "  global AC_count\n",
        "  AC_list = []\n",
        "  AC_count = 0\n",
        "  AC_final = False\n",
        "  Common_trigrams = False\n",
        "  Common_topic = False\n",
        "  word_count = 0\n",
        "\n",
        "  Trigram_counts = 0\n",
        "\n",
        "  A_trigrams = Complete_list_A[0]\n",
        "  C_trigram1 = Complete_list_C[0][0]\n",
        "  C_trigram2 = Complete_list_C[0][1]\n",
        "  C_trigram3 = Complete_list_C[0][2]\n",
        "  C_trigram4 = Complete_list_C[0][3]\n",
        "\n",
        "  if C_trigram1 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if C_trigram2 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if C_trigram3 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if C_trigram4 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if Trigram_counts > 1:\n",
        "    Common_trigrams = True\n",
        "    AC_list.append(Common_trigrams)\n",
        "\n",
        "  else:\n",
        "    AC_list.append(Common_trigrams)\n",
        "    AC_count = AC_count + 1\n",
        "\n",
        "\n",
        "#Standard deviation:\n",
        "  A1 = np.array(Complete_list_A[1])\n",
        "  C = np.array(Complete_list_C[1])\n",
        "  AC_Dev = A1-C\n",
        "\n",
        "  AC_list.append(list(AC_Dev))\n",
        "\n",
        "\n",
        "#Mean:\n",
        "  A1 = np.array(Complete_list_A[2])\n",
        "  C = np.array(Complete_list_C[2])\n",
        "  AC_Mean = A1-C\n",
        "\n",
        "\n",
        "  AC_list.append(list(AC_Mean))\n",
        "\n",
        "\n",
        "#Lexical Diversity:\n",
        "\n",
        "  A1 = np.array(Complete_list_A[3])\n",
        "  C = np.array(Complete_list_C[3])\n",
        "\n",
        "  AC_lex = A1-C\n",
        "\n",
        "  AC_list.append(AC_lex)\n",
        "\n",
        "\n",
        "\n",
        "#Function_words:\n",
        "  A1 = Complete_list_A[4]\n",
        "  C = Complete_list_C[4]\n",
        "  List = []\n",
        "  FunctionWord_Count = 0\n",
        "\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in C:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = C.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 3:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        FunctionWord_Count = FunctionWord_Count + 1\n",
        "\n",
        "  AC_list.append(FunctionWord_Count)\n",
        "\n",
        "\n",
        "\n",
        "#Punctuation:\n",
        "  A1 = Complete_list_A[5]\n",
        "  C = Complete_list_C[5]\n",
        "  List = []\n",
        "  Punctuation_Count = 0\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in C:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = C.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 2:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        Punctuation_Count = Punctuation_Count + 1\n",
        "\n",
        "  AC_list.append(Punctuation_Count)\n",
        "\n",
        "#Average sentence length:\n",
        "  A1 = np.array(Complete_list_A[6])\n",
        "  C = np.array(Complete_list_C[6])\n",
        "  AC_AvrgSents = A1-C\n",
        "\n",
        "  AC_list.append(AC_AvrgSents)\n",
        "\n",
        "\n",
        "#Number of sents:\n",
        "  A1 = np.array(Complete_list_A[7])\n",
        "  C = np.array(Complete_list_C[7])\n",
        "  AC_Sent_count = A1-C\n",
        "\n",
        "  AC_list.append(AC_Sent_count)\n",
        "\n",
        "#Number of words:\n",
        "  A1 = np.array(Complete_list_A[8])\n",
        "  C = np.array(Complete_list_C[8])\n",
        "  AC_Word_count= A1-C\n",
        "\n",
        "  AC_list.append(AC_Word_count)\n",
        "\n",
        "\n",
        "\n",
        "#TF-IDF score\n",
        "  TFIDF_AC = [] \n",
        "  TFIDF_AC.append(Feature_Data[0])\n",
        "  TFIDF_AC.append(Feature_Data[2])\n",
        "\n",
        "\n",
        "\n",
        "  TF_IDFvectorizer = TfidfVectorizer(strip_accents=\"unicode\", min_df = 0, stop_words='english')\n",
        "\n",
        "  word_tfidf_matrix = TF_IDFvectorizer.fit_transform(TFIDF_AC)\n",
        "\n",
        "  word_cosine_sim = cosine_similarity(word_tfidf_matrix, word_tfidf_matrix)\n",
        "\n",
        "  Docs = [\"A\", \"C\"]\n",
        "\n",
        "  AC_TFIDF = pd.DataFrame(word_cosine_sim,index=sorted(Docs),columns=sorted(Docs))\n",
        "\n",
        "  ACtfidf_Val = AC_TFIDF.iloc[1]['A']\n",
        "\n",
        "  AC_list.append(ACtfidf_Val)\n",
        "\n",
        "\n",
        "  \n",
        "#Jaccard Similarity\n",
        "  a = set(word_tokenize(Feature_Data[0])) \n",
        "  b = set(word_tokenize(Feature_Data[2])) \n",
        "  c = a.intersection(b)\n",
        "  \n",
        "  Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "  AC_list.append(Jaccard)\n",
        "\n",
        "\n",
        "# Keywords\n",
        "  DocA_words = keywords(Feature_Data[0], words=5, lemmatize=True).split(\"\\n\")\n",
        "  DocC_words = keywords(Feature_Data[2], words=5, lemmatize=True).split(\"\\n\")\n",
        "\n",
        "  A_keywords = []\n",
        "  B_keywords = []\n",
        "\n",
        "  word_count = 0\n",
        "\n",
        "  for word in DocA_words:\n",
        "      A_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for word in DocC_words:\n",
        "      B_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for each_word in A_keywords:\n",
        "    if each_word in B_keywords:\n",
        "        word_count = word_count + 1\n",
        "\n",
        "\n",
        "\n",
        "#Jaccard suspicious sentences:\n",
        "  DocA_sents = sent_tokenize(Feature_Data[0])\n",
        "  DocC_sents = sent_tokenize(Feature_Data[2])\n",
        "  sentJaccard_dict = {}\n",
        "  Total_sents = []\n",
        "\n",
        "  for original_sentences in DocA_sents:\n",
        "      Total_sents.append(original_sentences)\n",
        "\n",
        "  for sentences in DocC_sents:\n",
        "      Total_sents.append(sentences)\n",
        "\n",
        "  for sentenceA, sentenceB in itertools.combinations(Total_sents, 2):\n",
        "      a = set(word_tokenize(sentenceA)) \n",
        "      b = set(word_tokenize(sentenceB)) \n",
        "      c = a.intersection(b)\n",
        "    \n",
        "      Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "       # print(Jaccard)\n",
        "\n",
        "      if sentenceA != sentenceB and sentenceA not in DocC_sents and sentenceB not in DocA_sents:\n",
        "        if Jaccard > 0.4:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "      elif sentenceA and sentenceB in DocA_sents and DocC_sents:\n",
        "        if Jaccard > 0.4:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "\n",
        "  AC_list.append(sentJaccard_dict)\n",
        "\n",
        "#Word Mover's Distance:\n",
        "  W2V_data = []\n",
        "\n",
        "  D1 = word_tokenize(Feature_Data[0])\n",
        "  D2 = word_tokenize(Feature_Data[2])\n",
        "\n",
        "\n",
        "  W2V_data.append(D1)\n",
        "  W2V_data.append(D2)\n",
        "\n",
        "\n",
        "\n",
        "  Word2Vecmodel = gensim.models.Word2Vec(W2V_data, min_count = 0,  \n",
        "                                  size = 100, window = 5, sg =1) \n",
        "    \n",
        "  Word2Vecmodel.init_sims(replace=True) \n",
        "\n",
        "  #print(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[2]))\n",
        "  \n",
        "  AC_list.append(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[2]))\n",
        "\n",
        "\n",
        "#Topics:\n",
        "\n",
        "  if Complete_list_A[-1] == Complete_list_C[-1]:\n",
        "    Common_topic = True\n",
        "    AC_list.append(Common_topic)\n",
        "\n",
        "  else:\n",
        "    AC_list.append(Common_topic)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        " # print(AC_list)\n",
        "  #print(\"topic:\", Complete_list_C[-1])\n",
        "\n",
        "\n",
        "  if AC_list[0] == True:\n",
        "    AC_count = AC_count + 0.25\n",
        "\n",
        "  if -2 <= float(AC_list[1][0]) <= 2:\n",
        "    AC_count = AC_count + 0.25\n",
        "\n",
        "  if -2 <= float(AC_list[2][0]) <= 2:\n",
        "    AC_count = AC_count + 0.25\n",
        "\n",
        "  if -20 <= float(AC_list[3]) <= 20:\n",
        "    AC_count = AC_count + 0.25\n",
        "\n",
        "  if Punctuation_Count >= 2:\n",
        "    AC_count = AC_count + 0.25\n",
        "\n",
        "  if FunctionWord_Count >= 2:\n",
        "    AC_count = AC_count + 0.25\n",
        "  \n",
        "  if -2 <= AC_list[6] <= 2:\n",
        "    AC_count = AC_count + 0.25\n",
        "  \n",
        "  if -2 <= AC_list[7] <= 2:\n",
        "    AC_count = AC_count + 0.25\n",
        "\n",
        "  if -20 <= AC_list[8] <= 20:\n",
        "    AC_count = AC_count + 0.25\n",
        "\n",
        "  if AC_list[9] > 0.3:\n",
        "    AC_count = AC_count + 1.5\n",
        "\n",
        "  if AC_list[10] > 0.3:\n",
        "    AC_count = AC_count + 1.5\n",
        "\n",
        "  if word_count >= 2:\n",
        "    AC_count = AC_count + 1\n",
        "\n",
        "  if len(sentJaccard_dict) >= 1:\n",
        "    AC_count = AC_count + 1.5\n",
        "\n",
        "  if AC_list[12] < 0.1:\n",
        "    AC_count = AC_count + 1\n",
        "\n",
        "  if AC_list[-1] == True:\n",
        "    AC_count = AC_count + 0.5\n",
        "\n",
        "  \n",
        "  if AC_count > 5:\n",
        "    AC_final = True\n",
        "    Accused[Name_list[2]  + str(Name_tags)] = Name_list[0]\n",
        "\n",
        " # print(\"\\n\", AC_final, AC_count, \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def AD_comparisons():\n",
        "  global AD_list\n",
        "  global DocD_words\n",
        "  global AD_final\n",
        "  global AD_count\n",
        "  AD_list = []\n",
        "  AD_final = False\n",
        "  AD_count = 0\n",
        "  Common_trigrams = False\n",
        "  Common_topic = False\n",
        "  word_count = 0\n",
        "  Trigram_counts = 0\n",
        "\n",
        "  A_trigrams = Complete_list_A[0]\n",
        "  D_trigram1 = Complete_list_D[0][0]\n",
        "  D_trigram2 = Complete_list_D[0][1]\n",
        "  D_trigram3 = Complete_list_D[0][2]\n",
        "  D_trigram4 = Complete_list_D[0][3]\n",
        "\n",
        "  if D_trigram1 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if D_trigram2 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if D_trigram3 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if D_trigram4 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if Trigram_counts > 1:\n",
        "    Common_trigrams = True\n",
        "    AD_list.append(Common_trigrams)\n",
        "\n",
        "  else:\n",
        "    AD_list.append(Common_trigrams)\n",
        "    AD_count = AD_count + 1\n",
        "\n",
        "\n",
        "#Standard deviation:\n",
        "  A1 = np.array(Complete_list_A[1])\n",
        "  D = np.array(Complete_list_D[1])\n",
        "  AD_Dev = A1-D\n",
        "\n",
        "  AD_list.append(list(AD_Dev))\n",
        "\n",
        "\n",
        "#Mean:\n",
        "  A1 = np.array(Complete_list_A[2])\n",
        "  D = np.array(Complete_list_D[2])\n",
        "  AD_Mean = A1-D\n",
        "\n",
        "\n",
        "  AD_list.append(list(AD_Mean))\n",
        "\n",
        "\n",
        "#Lexical Diversity:\n",
        "\n",
        "  A1 = np.array(Complete_list_A[3])\n",
        "  D = np.array(Complete_list_D[3])\n",
        "\n",
        "  AD_lex = A1-D\n",
        "\n",
        "  AD_list.append(AD_lex)\n",
        "\n",
        "\n",
        "\n",
        "#Function_words:\n",
        "  A1 = Complete_list_A[4]\n",
        "  D = Complete_list_D[4]\n",
        "  List = []\n",
        "  FunctionWord_Count = 0\n",
        "\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in D:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = D.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 3:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        FunctionWord_Count = FunctionWord_Count + 1\n",
        "\n",
        "  AD_list.append(FunctionWord_Count)\n",
        "\n",
        "\n",
        "\n",
        "#Punctuation:\n",
        "  A1 = Complete_list_A[5]\n",
        "  D = Complete_list_D[5]\n",
        "  List = []\n",
        "  Punctuation_Count = 0\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in D:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = D.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 2:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        Punctuation_Count = Punctuation_Count + 1\n",
        "\n",
        "  AD_list.append(Punctuation_Count)\n",
        "\n",
        "\n",
        "#Average sentence length:\n",
        "  A1 = np.array(Complete_list_A[6])\n",
        "  D = np.array(Complete_list_D[6])\n",
        "  AD_AvrgSents = A1-D\n",
        "\n",
        "  AD_list.append(AD_AvrgSents)\n",
        "\n",
        "\n",
        "#Number of sents:\n",
        "  A1 = np.array(Complete_list_A[7])\n",
        "  D = np.array(Complete_list_D[7])\n",
        "  AD_Sent_count = A1-D\n",
        "\n",
        "  AD_list.append(AD_Sent_count)\n",
        "\n",
        "#Number of words:\n",
        "  A1 = np.array(Complete_list_A[8])\n",
        "  D = np.array(Complete_list_D[8])\n",
        "  AD_Word_count= A1-D\n",
        "\n",
        "  AD_list.append(AD_Word_count)\n",
        "\n",
        "\n",
        "#TF-IDF score\n",
        "  TFIDF_AD = [] \n",
        "  TFIDF_AD.append(Feature_Data[0])\n",
        "  TFIDF_AD.append(Feature_Data[3])\n",
        "\n",
        "\n",
        "\n",
        "  TF_IDFvectorizer = TfidfVectorizer(strip_accents=\"unicode\", min_df = 0, stop_words='english')\n",
        "\n",
        "  word_tfidf_matrix = TF_IDFvectorizer.fit_transform(TFIDF_AD)\n",
        "\n",
        "  word_cosine_sim = cosine_similarity(word_tfidf_matrix, word_tfidf_matrix)\n",
        "\n",
        "  Docs = [\"A\", \"D\"]\n",
        "\n",
        "  AD_TFIDF = pd.DataFrame(word_cosine_sim,index=sorted(Docs),columns=sorted(Docs))\n",
        "\n",
        "  ADtfidf_Val = AD_TFIDF.iloc[1]['A']\n",
        "\n",
        "  AD_list.append(ADtfidf_Val)\n",
        "\n",
        "\n",
        "  \n",
        "#Jaccard Similarity\n",
        "  a = set(word_tokenize(Feature_Data[0])) \n",
        "  b = set(word_tokenize(Feature_Data[3])) \n",
        "  c = a.intersection(b)\n",
        "  \n",
        "  Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "  AD_list.append(Jaccard)\n",
        "\n",
        "\n",
        "# Keywords\n",
        "  DocA_words = keywords(Feature_Data[0], words=5, lemmatize=True).split(\"\\n\")\n",
        "  DocD_words = keywords(Feature_Data[3], words=5, lemmatize=True).split(\"\\n\")\n",
        "\n",
        "  A_keywords = []\n",
        "  B_keywords = []\n",
        "\n",
        "  word_count = 0\n",
        "\n",
        "  for word in DocA_words:\n",
        "      A_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for word in DocD_words:\n",
        "      B_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for each_word in A_keywords:\n",
        "    if each_word in B_keywords:\n",
        "        word_count = word_count + 1\n",
        "\n",
        "\n",
        "\n",
        "#Jaccard suspicious sentences:\n",
        "  DocA_sents = sent_tokenize(Feature_Data[0])\n",
        "  DocD_sents = sent_tokenize(Feature_Data[3])\n",
        "  sentJaccard_dict = {}\n",
        "  Total_sents = []\n",
        "\n",
        "  for original_sentences in DocA_sents:\n",
        "      Total_sents.append(original_sentences)\n",
        "\n",
        "  for sentences in DocD_sents:\n",
        "      Total_sents.append(sentences)\n",
        "\n",
        "  for sentenceA, sentenceB in itertools.combinations(Total_sents, 2):\n",
        "      a = set(word_tokenize(sentenceA)) \n",
        "      b = set(word_tokenize(sentenceB)) \n",
        "      c = a.intersection(b)\n",
        "    \n",
        "      Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "       # print(Jaccard)\n",
        "\n",
        "      if sentenceA != sentenceB and sentenceA not in DocD_sents and sentenceB not in DocA_sents:\n",
        "        if Jaccard > 0.4:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "      elif sentenceA and sentenceB in DocA_sents and DocD_sents:\n",
        "        if Jaccard > 0.4:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "  AD_list.append(sentJaccard_dict)\n",
        "\n",
        "#Word Mover's Distance:\n",
        "  W2V_data = []\n",
        "\n",
        "  D1 = word_tokenize(Feature_Data[0])\n",
        "  D2 = word_tokenize(Feature_Data[3])\n",
        "\n",
        "\n",
        "  W2V_data.append(D1)\n",
        "  W2V_data.append(D2)\n",
        "\n",
        "\n",
        "\n",
        "  Word2Vecmodel = gensim.models.Word2Vec(W2V_data, min_count = 0,  \n",
        "                                  size = 100, window = 5, sg =1) \n",
        "    \n",
        "  Word2Vecmodel.init_sims(replace=True) \n",
        "\n",
        "  #print(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[3]))\n",
        "  \n",
        "  AD_list.append(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[3]))\n",
        "\n",
        "\n",
        "#Topics:\n",
        "\n",
        "  if Complete_list_A[-1] == Complete_list_D[-1]:\n",
        "    Common_topic = True\n",
        "    AD_list.append(Common_topic)\n",
        "\n",
        "  else:\n",
        "    AD_list.append(Common_topic)\n",
        "\n",
        "\n",
        " # print(AD_list)\n",
        "\n",
        " # print(\"topic:\", Complete_list_D[-1])\n",
        "\n",
        "  if AD_list[0] == True:\n",
        "    AD_count = AD_count + 0.25\n",
        "\n",
        "  if -2 <= float(AD_list[1][0]) <= 2:\n",
        "    AD_count = AD_count + 0.25\n",
        "\n",
        "  if -2 <= float(AD_list[2][0]) <= 2:\n",
        "    AD_count = AD_count + 0.25\n",
        "\n",
        "  if -20 <= AD_list[3] <= 20:\n",
        "    AD_count = AD_count + 0.25\n",
        "\n",
        "  if Punctuation_Count >= 2:\n",
        "    AD_count = AD_count + 0.25\n",
        "\n",
        "  if FunctionWord_Count >= 2:\n",
        "    AD_count = AD_count + 0.25\n",
        "\n",
        "  if -2 <= AD_list[6] <= 2:\n",
        "    AD_count = AD_count + 0.25\n",
        "  \n",
        "  if -3 <= AD_list[7] <= 3:\n",
        "    AD_count = AD_count + 0.25\n",
        "\n",
        "  if -20 <= AD_list[8] <= 20:\n",
        "    AD_count = AD_count + 0.25\n",
        "\n",
        "  if AD_list[9] > 0.3:\n",
        "    AD_count = AD_count + 1.5\n",
        "\n",
        "  if AD_list[10] > 0.3:\n",
        "    AD_count = AD_count + 1.5\n",
        "\n",
        "  if word_count >= 2:\n",
        "    AD_count = AD_count + 1\n",
        "\n",
        "  if len(sentJaccard_dict) >= 1:\n",
        "    AD_count = AD_count + 1.5\n",
        "\n",
        "  if AD_list[12] < 0.1:\n",
        "    AD_count = AD_count + 1\n",
        "\n",
        "  if AD_list[-1] == True:\n",
        "    AD_count = AD_count + 0.5\n",
        "\n",
        "  \n",
        "  if AD_count > 5:\n",
        "    AD_final = True\n",
        "    Accused[Name_list[3] + str(Name_tags)] = Name_list[0]\n",
        "\n",
        " # print(\"\\n\", AD_final, AD_count, \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def AE_comparisons():\n",
        "  global AE_list\n",
        "  global DocE_words\n",
        "  global AE_final\n",
        "  global AE_count\n",
        "  AE_list = []\n",
        "  word_count = 0\n",
        "  AE_final = False\n",
        "  AE_count = 0\n",
        "  Common_trigrams = False\n",
        "  Common_topic = False\n",
        "\n",
        "  Trigram_counts = 0\n",
        "\n",
        "  A_trigrams = Complete_list_A[0]\n",
        "  E_trigram1 = Complete_list_E[0][0]\n",
        "  E_trigram2 = Complete_list_E[0][1]\n",
        "  E_trigram3 = Complete_list_E[0][2]\n",
        "  E_trigram4 = Complete_list_E[0][3]\n",
        "\n",
        "  if E_trigram1 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  if E_trigram2 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if E_trigram3 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "\n",
        "  if E_trigram4 in A_trigrams:\n",
        "    Trigram_counts = Trigram_counts + 1\n",
        "  \n",
        "  \n",
        "\n",
        "  if Trigram_counts > 1:\n",
        "    Common_trigrams = True\n",
        "    AE_list.append(Common_trigrams)\n",
        "\n",
        "  else:\n",
        "    AE_list.append(Common_trigrams)\n",
        "    AE_count = AE_count + 1\n",
        "\n",
        "\n",
        "#Standard deviation:\n",
        "  A1 = np.array(Complete_list_A[1])\n",
        "  E = np.array(Complete_list_E[1])\n",
        "  AE_Dev = A1-E\n",
        "\n",
        "  AE_list.append(list(AE_Dev))\n",
        "\n",
        "\n",
        "#Mean:\n",
        "  A1 = np.array(Complete_list_A[2])\n",
        "  E = np.array(Complete_list_E[2])\n",
        "  AE_Mean = A1-E\n",
        "\n",
        "\n",
        "  AE_list.append(list(AE_Mean))\n",
        "\n",
        "\n",
        "#Lexical Diversity:\n",
        "\n",
        "  A1 = np.array(Complete_list_A[3])\n",
        "  E = np.array(Complete_list_E[3])\n",
        "\n",
        "  AE_lex = A1-E\n",
        "\n",
        "  AE_list.append(AE_lex)\n",
        "\n",
        "\n",
        "\n",
        "#Function_words:\n",
        "  A1 = Complete_list_A[4]\n",
        "  E = Complete_list_E[4]\n",
        "  List = []\n",
        "  FunctionWord_Count = 0\n",
        "\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in E:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = E.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 3:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        FunctionWord_Count = FunctionWord_Count + 1\n",
        "\n",
        "  AE_list.append(FunctionWord_Count)\n",
        "\n",
        "\n",
        "\n",
        "#Punctuation:\n",
        "  A1 = Complete_list_A[5]\n",
        "  E = Complete_list_E[5]\n",
        "  List = []\n",
        "  Punctuation_Count = 0\n",
        "\n",
        "  for keys in A1:\n",
        "    if keys in E:\n",
        "      Val1 = A1.get(keys)\n",
        "      Val2 = E.get(keys)\n",
        "      List.append(Val1 - Val2)\n",
        "\n",
        "\n",
        "  if len(List) >= 2:\n",
        "    for differences in List:\n",
        "      if differences == 0 or 1 or 2 or -1 or -2:\n",
        "        Punctuation_Count = Punctuation_Count + 1\n",
        "\n",
        "  AE_list.append(Punctuation_Count)\n",
        "\n",
        "\n",
        "#Average sentence length:\n",
        "  A1 = np.array(Complete_list_E[6])\n",
        "  E = np.array(Complete_list_E[6])\n",
        "  AE_AvrgSents = A1-E\n",
        "\n",
        "  AE_list.append(AE_AvrgSents)\n",
        "\n",
        "\n",
        "#Number of sents:\n",
        "  A1 = np.array(Complete_list_A[7])\n",
        "  E = np.array(Complete_list_E[7])\n",
        "  AE_Sent_count = A1-E\n",
        "\n",
        "  AE_list.append(AE_Sent_count)\n",
        "\n",
        "#Number of words:\n",
        "  A1 = np.array(Complete_list_A[8])\n",
        "  E = np.array(Complete_list_E[8])\n",
        "  AE_Word_count= A1-E\n",
        "\n",
        "  AE_list.append(AE_Word_count)\n",
        "\n",
        "\n",
        "#TF-IDF score\n",
        "  TFIDF_AE = [] \n",
        "  TFIDF_AE.append(Feature_Data[0])\n",
        "  TFIDF_AE.append(Feature_Data[4])\n",
        "\n",
        "\n",
        "\n",
        "  TF_IDFvectorizer = TfidfVectorizer(strip_accents=\"unicode\", min_df = 0, stop_words='english')\n",
        "\n",
        "  word_tfidf_matrix = TF_IDFvectorizer.fit_transform(TFIDF_AE)\n",
        "\n",
        "  word_cosine_sim = cosine_similarity(word_tfidf_matrix, word_tfidf_matrix)\n",
        "\n",
        "  Docs = [\"A\", \"E\"]\n",
        "\n",
        "  AE_TFIDF = pd.DataFrame(word_cosine_sim,index=sorted(Docs),columns=sorted(Docs))\n",
        "\n",
        "  AEtfidf_Val = AE_TFIDF.iloc[1]['A']\n",
        "\n",
        "  AE_list.append(AEtfidf_Val)\n",
        "\n",
        "\n",
        "  \n",
        "#Jaccard Similarity\n",
        "  a = set(word_tokenize(Feature_Data[0])) \n",
        "  b = set(word_tokenize(Feature_Data[4])) \n",
        "  c = a.intersection(b)\n",
        "  \n",
        "  Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "  AE_list.append(Jaccard)\n",
        "\n",
        "\n",
        "# Keywords\n",
        "  DocA_words = keywords(Feature_Data[0], words=5, lemmatize=True).split(\"\\n\")\n",
        "  DocE_words = keywords(Feature_Data[4], words=5, lemmatize=True).split(\"\\n\")\n",
        "\n",
        "  A_keywords = []\n",
        "  B_keywords = []\n",
        "\n",
        "  word_count = 0\n",
        "\n",
        "  for word in DocA_words:\n",
        "      A_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for word in DocE_words:\n",
        "      B_keywords.append(preprocess(word_tokenize(word)))\n",
        "\n",
        "  for each_word in A_keywords:\n",
        "    if each_word in B_keywords:\n",
        "        word_count = word_count + 1\n",
        "\n",
        "\n",
        "\n",
        "#Jaccard suspicious sentences:\n",
        "  DocA_sents = sent_tokenize(Feature_Data[0])\n",
        "  DocE_sents = sent_tokenize(Feature_Data[4])\n",
        "  sentJaccard_dict = {}\n",
        "  Total_sents = []\n",
        "\n",
        "  for original_sentences in DocA_sents:\n",
        "      Total_sents.append(original_sentences)\n",
        "\n",
        "  for sentences in DocE_sents:\n",
        "      Total_sents.append(sentences)\n",
        "\n",
        "  for sentenceA, sentenceB in itertools.combinations(Total_sents, 2):\n",
        "      a = set(word_tokenize(sentenceA)) \n",
        "      b = set(word_tokenize(sentenceB)) \n",
        "      c = a.intersection(b)\n",
        "    \n",
        "      Jaccard = float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "       # print(Jaccard)\n",
        "\n",
        "      if sentenceA != sentenceB and sentenceA not in DocE_sents and sentenceB not in DocA_sents:\n",
        "        if Jaccard > 0.4:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "      elif sentenceA and sentenceB in DocA_sents and DocE_sents:\n",
        "        if Jaccard > 0.4:\n",
        "          sentJaccard_dict[sentenceB] = sentenceA\n",
        "\n",
        "\n",
        "  AE_list.append(sentJaccard_dict)\n",
        "\n",
        "#Word Mover's Distance:\n",
        "  W2V_data = []\n",
        "\n",
        "  D1 = word_tokenize(Feature_Data[0])\n",
        "  D2 = word_tokenize(Feature_Data[4])\n",
        "\n",
        "\n",
        "  W2V_data.append(D1)\n",
        "  W2V_data.append(D2)\n",
        "\n",
        "\n",
        "\n",
        "  Word2Vecmodel = gensim.models.Word2Vec(W2V_data, min_count = 0,  \n",
        "                                  size = 100, window = 5, sg =1) \n",
        "    \n",
        "  Word2Vecmodel.init_sims(replace=True) \n",
        "\n",
        "  #print(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[4]))\n",
        "  \n",
        "  AE_list.append(Word2Vecmodel.wmdistance(Feature_Data[0], Feature_Data[4]))\n",
        "\n",
        "\n",
        "\n",
        "#Topics:\n",
        "\n",
        "  if Complete_list_A[-1] == Complete_list_E[-1]:\n",
        "    Common_topic = True\n",
        "    AE_list.append(Common_topic)\n",
        "\n",
        "  else:\n",
        "    AE_list.append(Common_topic)\n",
        "\n",
        "\n",
        "  print(AE_list)\n",
        "\n",
        "  if AE_list[0] == True:\n",
        "    AE_count = AE_count + 0.25\n",
        "\n",
        "  if -2 <= float(AE_list[1][0]) <= 2:\n",
        "    AE_count = AE_count + 0.25\n",
        "\n",
        "  if -2 <= float(AE_list[2][0]) <= 2:\n",
        "    AE_count = AE_count + 0.25\n",
        "\n",
        "  if -20 <= AE_list[3] <= 20:\n",
        "    AE_count = AE_count + 0.25\n",
        "\n",
        "  if Punctuation_Count >= 2:\n",
        "    AE_count = AE_count + 0.25\n",
        "\n",
        "  if FunctionWord_Count >= 2:\n",
        "    AE_count = AE_count + 0.25\n",
        "\n",
        "  if -2 <= AE_list[6] <= 2:\n",
        "    AE_count = AE_count + 0.25\n",
        "  \n",
        "  if -2 <= AE_list[7] <= 2:\n",
        "    AE_count = AE_count + 0.25\n",
        "\n",
        "  if -20 <= AE_list[8] <= 20:\n",
        "    AE_count = AE_count + 0.25\n",
        "\n",
        "  if AE_list[9] > 0.3:\n",
        "    AE_count = AE_count + 1.5\n",
        "\n",
        "  if AE_list[10] > 0.3:\n",
        "    AE_count = AE_count + 1.5\n",
        "\n",
        "  if word_count >= 1:\n",
        "    AE_count = AE_count + 1\n",
        "\n",
        "  if len(sentJaccard_dict) >= 1:\n",
        "    AE_count = AE_count + 1.5\n",
        "\n",
        "  if AE_list[12] < 0.1:\n",
        "    AE_count = AE_count + 1.5\n",
        "\n",
        "  if AE_list[-1] == True:\n",
        "    AE_count = AE_count + 0.5\n",
        "\n",
        "  \n",
        "  if AE_count > 5:\n",
        "    AE_final = True\n",
        "    Accused[Name_list[4] + str(Name_tags)] = Name_list[0]\n",
        "\n",
        " # print(\"topic:\", Complete_list_E[-1])\n",
        " # print(\"\\n\", AE_final, AE_count, \"\\n\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV8KJrlwSPvj"
      },
      "source": [
        "# Define \"Graphing\" Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnoMw2FUxn2h"
      },
      "source": [
        "def Graph1():\n",
        "  # set heights of bars\n",
        "  StudentA_Vals = (Complete_list_A[2][0], Complete_list_A[3],)\n",
        "  StudentB_Vals = (Complete_list_B[2][0], Complete_list_B[3])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    StudentC_Vals = (Complete_list_C[2][0], Complete_list_C[3])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    StudentD_Vals = (Complete_list_D[2][0], Complete_list_D[3])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    StudentE_Vals = (Complete_list_E[2][0], Complete_list_E[3])\n",
        "\n",
        "\n",
        "  \n",
        "  # Set position of bar on X axis\n",
        "  r1 = np.arange(len(StudentA_Vals))\n",
        "  r2 = [x + barWidth for x in r1]\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    r3 = [x + barWidth for x in r2]\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    r4 = [x + barWidth for x in r3]\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    r5 = [x + barWidth for x in r4]\n",
        "\n",
        "\n",
        "  \n",
        "  # Make the plot\n",
        "  plt.bar(r1, StudentA_Vals, color='b', width=barWidth, edgecolor='white', label='Student A')\n",
        "  plt.bar(r2, StudentB_Vals, color='r', width=barWidth, edgecolor='white', label='Student B')\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    plt.bar(r3, StudentC_Vals, color='g', width=barWidth, edgecolor='white', label='Student C')\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    plt.bar(r4, StudentD_Vals, color='y', width=barWidth, edgecolor='white', label='Student D')\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    plt.bar(r5, StudentE_Vals, color='m', width=barWidth, edgecolor='white', label='Student E')\n",
        "\n",
        "  \n",
        "  # Add xticks on the middle of the group bars\n",
        "  plt.xlabel('Features', fontweight='bold')\n",
        "  plt.ylabel('Length/Count', fontweight='bold')\n",
        "  plt.xticks([r + barWidth for r in range(len(StudentA_Vals))], ['Means', 'Unique Words'])\n",
        "  \n",
        "  # Create legend & Show graphic\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.show()\n",
        "\n",
        "  print(\"StudentA: \\n \", Name_list[0])\n",
        "  print(\"StudentB: \\n \", Name_list[1])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    print(\"StudentC: \\n \", Name_list[2])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    print(\"StudentD: \\n \", Name_list[3])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    print(\"StudentE: \\n \", Name_list[4])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Graph2():\n",
        "  # set heights of bars\n",
        "  StudentA_Vals = (Complete_list_A[8])\n",
        "  StudentB_Vals = (Complete_list_B[8])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    StudentC_Vals = (Complete_list_C[8])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    StudentD_Vals = (Complete_list_D[8])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    StudentE_Vals = (Complete_list_E[8])\n",
        "\n",
        "\n",
        "  \n",
        "  # Set position of bar on X axis\n",
        "  r1 = np.arange(1)\n",
        "  r2 = [x + barWidth for x in r1]\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    r3 = [x + barWidth for x in r2]\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    r4 = [x + barWidth for x in r3]\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    r5 = [x + barWidth for x in r4]\n",
        "\n",
        "\n",
        "  \n",
        "  # Make the plot\n",
        "  plt.bar(r1, StudentA_Vals, color='b', width=barWidth, edgecolor='white', label='Student A')\n",
        "  plt.bar(r2, StudentB_Vals, color='r', width=barWidth, edgecolor='white', label='Student B')\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    plt.bar(r3, StudentC_Vals, color='g', width=barWidth, edgecolor='white', label='Student C')\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    plt.bar(r4, StudentD_Vals, color='y', width=barWidth, edgecolor='white', label='Student D')\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    plt.bar(r5, StudentE_Vals, color='m', width=barWidth, edgecolor='white', label='Student E')\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  # Add xticks on the middle of the group bars\n",
        "  plt.xlabel('Features', fontweight='bold')\n",
        "  plt.ylabel('Length/Count', fontweight='bold')\n",
        "  plt.xticks([r + barWidth for r in range(1)], ['Num of words'])\n",
        "  \n",
        "          \n",
        "\n",
        "  # Create legend & Show graphic\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  print(\"StudentA: \\n \", Name_list[0])\n",
        "  print(\"StudentB: \\n \", Name_list[1])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    print(\"StudentC: \\n \", Name_list[2])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    print(\"StudentD: \\n \", Name_list[3])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    print(\"StudentE: \\n \", Name_list[4])\n",
        "\n",
        "\n",
        "def Graph3():\n",
        "  \n",
        "  StudentA_Vals = (Complete_list_A[1][0], Complete_list_A[7])\n",
        "  StudentB_Vals = (Complete_list_B[1][0], Complete_list_B[7])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    StudentC_Vals = (Complete_list_C[1][0], Complete_list_C[7])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    StudentD_Vals = (Complete_list_D[1][0], Complete_list_D[7])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    StudentE_Vals = (Complete_list_E[1][0], Complete_list_E[7])\n",
        "\n",
        "\n",
        "  \n",
        "  # Set position of bar on X axis\n",
        "  r1 = np.arange(len(StudentA_Vals))\n",
        "  r2 = [x + barWidth for x in r1]\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    r3 = [x + barWidth for x in r2]\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    r4 = [x + barWidth for x in r3]\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    r5 = [x + barWidth for x in r4]\n",
        "\n",
        "\n",
        "  \n",
        "  # Make the plot\n",
        "  plt.bar(r1, StudentA_Vals, color='b', width=barWidth, edgecolor='white', label='Student A')\n",
        "  plt.bar(r2, StudentB_Vals, color='r', width=barWidth, edgecolor='white', label='Student B')\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    plt.bar(r3, StudentC_Vals, color='g', width=barWidth, edgecolor='white', label='Student C')\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    plt.bar(r4, StudentD_Vals, color='y', width=barWidth, edgecolor='white', label='Student D')\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    plt.bar(r5, StudentE_Vals, color='m', width=barWidth, edgecolor='white', label='Student E')\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  # Add xticks on the middle of the group bars\n",
        "  plt.xlabel('Features', fontweight='bold')\n",
        "  plt.ylabel('Length/Count', fontweight='bold')\n",
        "  plt.xticks([r + barWidth for r in range(len(StudentA_Vals))], ['Deviation', 'Number of Sentences'])\n",
        "  \n",
        "          \n",
        "\n",
        "  # Create legend & Show graphic\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  print(\"StudentA: \\n \", Name_list[0])\n",
        "  print(\"StudentB: \\n \", Name_list[1])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    print(\"StudentC: \\n \", Name_list[2])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    print(\"StudentD: \\n \", Name_list[3])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    print(\"StudentE: \\n \", Name_list[4])\n",
        "\n",
        "\n",
        "def Graph4():\n",
        "  # set heights of bars\n",
        "  StudentA_Vals = (Complete_list_A[6])\n",
        "  StudentB_Vals = (Complete_list_B[6])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    StudentC_Vals = (Complete_list_C[6])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    StudentD_Vals = (Complete_list_D[6])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    StudentE_Vals = (Complete_list_E[6])\n",
        "\n",
        "\n",
        "  \n",
        "  # Set position of bar on X axis\n",
        "  r1 = np.arange(1)\n",
        "  r2 = [x + barWidth for x in r1]\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    r3 = [x + barWidth for x in r2]\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    r4 = [x + barWidth for x in r3]\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    r5 = [x + barWidth for x in r4]\n",
        "\n",
        "\n",
        "  \n",
        "  # Make the plot\n",
        "  plt.bar(r1, StudentA_Vals, color='b', width=barWidth, edgecolor='white', label='Student A')\n",
        "  plt.bar(r2, StudentB_Vals, color='r', width=barWidth, edgecolor='white', label='Student B')\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    plt.bar(r3, StudentC_Vals, color='g', width=barWidth, edgecolor='white', label='Student C')\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    plt.bar(r4, StudentD_Vals, color='y', width=barWidth, edgecolor='white', label='Student D')\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    plt.bar(r5, StudentE_Vals, color='m', width=barWidth, edgecolor='white', label='Student E')\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  # Add xticks on the middle of the group bars\n",
        "  plt.xlabel('Features', fontweight='bold')\n",
        "  plt.ylabel('Length/Count', fontweight='bold')\n",
        "  plt.xticks([r + barWidth for r in range(1)], ['Average Length of Sents'])\n",
        "  \n",
        "          \n",
        "\n",
        "  # Create legend & Show graphic\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  print(\"StudentA: \\n \", Name_list[0])\n",
        "  print(\"StudentB: \\n \", Name_list[1])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    print(\"StudentC: \\n \", Name_list[2])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    print(\"StudentD: \\n \", Name_list[3])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    print(\"StudentE: \\n \", Name_list[4])\n",
        "\n",
        "\n",
        "def SimilarityMetricsGraph():\n",
        "  # set heights of bars\n",
        "  AB_vals = (AB_list[9], AB_list[10])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    AC_vals = (AC_list[9], AC_list[10])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    AD_vals = (AD_list[9], AD_list[10])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    AE_vals = (AE_list[9], AE_list[10])\n",
        "  \n",
        "  # Set position of bar on X axis\n",
        "  r1 = np.arange(len(AB_vals))\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    r2 = [x + barWidth for x in r1]\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    r3 = [x + barWidth for x in r2]\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    r4 = [x + barWidth for x in r3]\n",
        "\n",
        "\n",
        "  \n",
        "  # Make the plot\n",
        "  plt.bar(r1, AB_vals, color='b', width=barWidth, edgecolor='white', label='AB')\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    plt.bar(r2, AC_vals, color='r', width=barWidth, edgecolor='white', label='AC')\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    plt.bar(r3, AD_vals, color='y', width=barWidth, edgecolor='white', label='AD')\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    plt.bar(r4, AE_vals, color='m', width=barWidth, edgecolor='white', label='AE')\n",
        "\n",
        "  \n",
        "  # Add xticks on the middle of the group bars\n",
        "  plt.xlabel('Similarity Values', fontweight='bold')\n",
        "  plt.ylabel('Score', fontweight='bold')\n",
        "  plt.xticks([r + barWidth for r in range(len(AB_vals))], ['TF-IDF', 'Jaccard'])\n",
        "  \n",
        "          \n",
        "\n",
        "  # Create legend & Show graphic\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  print(\"StudentA: \\n \", Name_list[0])\n",
        "  print(\"StudentB: \\n \", Name_list[1])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    print(\"StudentC: \\n \", Name_list[2])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    print(\"StudentD: \\n \", Name_list[3])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    print(\"StudentE: \\n \", Name_list[4])\n",
        "\n",
        "\n",
        "\n",
        "def SimilarityMetricsGraphWMD():\n",
        "  WMD = [1]\n",
        "  AB_vals = (AB_list[12])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    AC_vals = (AC_list[12])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    AD_vals = (AD_list[12])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    AE_vals = (AE_list[12])\n",
        "  \n",
        "  # Set position of bar on X axis\n",
        "  r1 = np.arange(1)\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    r2 = [x + barWidth for x in r1]\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    r3 = [x + barWidth for x in r2]\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    r4 = [x + barWidth for x in r3]\n",
        "\n",
        "\n",
        "  \n",
        "  # Make the plot\n",
        "  plt.bar(r1, AB_vals, color='b', width=barWidth, edgecolor='white', label='AB')\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    plt.bar(r2, AC_vals, color='r', width=barWidth, edgecolor='white', label='AC')\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    plt.bar(r3, AD_vals, color='y', width=barWidth, edgecolor='white', label='AD')\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    plt.bar(r4, AE_vals, color='m', width=barWidth, edgecolor='white', label='AE')\n",
        "\n",
        "  \n",
        "  # Add xticks on the middle of the group bars\n",
        "  plt.xlabel(\"Word Mover's Distance\", fontweight='bold')\n",
        "  plt.ylabel('Distance', fontweight='bold')\n",
        "  plt.xticks([r + barWidth for r in range(len(WMD))], ['WMD'])\n",
        "  \n",
        "          \n",
        "\n",
        "  # Create legend & Show graphic\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  print(\"StudentA: \\n \", Name_list[0])\n",
        "  print(\"StudentB: \\n \", Name_list[1])\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    print(\"StudentC: \\n \", Name_list[2])\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    print(\"StudentD: \\n \", Name_list[3])\n",
        "\n",
        "  if len(Feature_Data) >= 5:\n",
        "    print(\"StudentE: \\n \", Name_list[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWKu1sBppS1j"
      },
      "source": [
        "# Enter Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZEkEf9REQs4"
      },
      "source": [
        "Data = []\n",
        "LDA_Data = []\n",
        "Test = []\n",
        "Names = []\n",
        "NamesLDA = {}\n",
        "Accused = {}\n",
        "global barWidth\n",
        "barWidth = 0.1\n",
        "global Name_tags\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "  Name = builtins.input(\"enter name     \")\n",
        "  Text = builtins.input(\"enter text or 'q' to quit    \")\n",
        "  \n",
        "  if Text == \"q\":\n",
        "   # Test_name = builtins.input(\"Enter Name For Text     \")\n",
        "   # Test_docNum = Names.index(Test_name)\n",
        "   # Test.append(Data[Test_docNum])\n",
        "    break\n",
        "\n",
        "  else:\n",
        "    Data.append(Text)\n",
        "    LDA_Data.append(word_tokenize(Text))\n",
        "\n",
        "    Names.append(Name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lATJ0OoR7NMc"
      },
      "source": [
        "# Loop Starts Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGe0GfetQytg"
      },
      "source": [
        "Name_tags = 0\n",
        "Accused = {}\n",
        "global sentJaccard_dict\n",
        "global Total_sents\n",
        "\n",
        "for every_student in Names:\n",
        "  print(\"NEW ITERATION SELECTING \", every_student, \" AS STUDENT A \\n ------------------ \\n\", )\n",
        "  Test = []\n",
        "\n",
        "  Test_docNum = Names.index(every_student)\n",
        "  Test.append(Data[Test_docNum])\n",
        "\n",
        "\n",
        "  Suspects_Student_X = {}\n",
        "\n",
        "  for docs in range(len(Data)):\n",
        "      tokens = nltk.word_tokenize(Data[docs])\n",
        "\n",
        "      # Filter out punctuation\n",
        "      Doc_tokens[docs] = ([token for token in tokens\n",
        "                                              if any(c.isalpha() for c in token)])\n",
        "\n",
        "      # Get a distribution of token lengths\n",
        "      token_lengths = [len(token) for token in Doc_tokens[docs]]\n",
        "\n",
        "\n",
        "\n",
        "  for docs in range(len(Data)):\n",
        "      Doc_tokens[docs] = (\n",
        "          [tok.lower() for tok in Doc_tokens[docs]])\n",
        "\n",
        "  # Combine every paper except our test case into a single corpus\n",
        "  whole_corpus = []\n",
        "  for docs in range(len(Data)):\n",
        "      whole_corpus += Doc_tokens[docs]\n",
        "\n",
        "  # Get a frequency distribution\n",
        "  whole_corpus_freq_dist = list(nltk.FreqDist(whole_corpus).most_common(30))\n",
        "  whole_corpus_freq_dist[ :10 ]\n",
        "\n",
        "  features = [word for word,freq in whole_corpus_freq_dist]\n",
        "  feature_freqs = {}\n",
        "\n",
        "  for docs in range(len(Data)):\n",
        "      # A dictionary for each candidate's features\n",
        "      feature_freqs[docs] = {}\n",
        "\n",
        "      # A helper value containing the number of tokens in the author's subcorpus\n",
        "      overall = len(Doc_tokens[docs])\n",
        "\n",
        "      # Calculate each feature's presence in the subcorpus\n",
        "      for feature in features:\n",
        "          presence = Doc_tokens[docs].count(feature)\n",
        "          feature_freqs[docs][feature] = presence / overall\n",
        "\n",
        "\n",
        "  # The data structure into which we will be storing the \"corpus standard\" statistics\n",
        "  corpus_features = {}\n",
        "\n",
        "  # For each feature...\n",
        "  for feature in features:\n",
        "      # Create a sub-dictionary that will contain the feature's mean\n",
        "      # and standard deviation\n",
        "      corpus_features[feature] = {}\n",
        "\n",
        "      # Calculate the mean of the frequencies expressed in the subcorpora\n",
        "      feature_average = 0\n",
        "      for docs in range(len(Data)):\n",
        "          feature_average += feature_freqs[docs][feature]\n",
        "      feature_average /= len(Data)\n",
        "      corpus_features[feature][\"Mean\"] = feature_average\n",
        "\n",
        "      # Calculate the standard deviation using the basic formula for a sample\n",
        "      feature_stdev = 0\n",
        "      for docs in range(len(Data)):\n",
        "          diff = feature_freqs[docs][feature] - corpus_features[feature][\"Mean\"]\n",
        "          feature_stdev += diff*diff\n",
        "      feature_stdev /= (len(Data) - 1)\n",
        "      feature_stdev = math.sqrt(feature_stdev)\n",
        "      corpus_features[feature][\"StdDev\"] = feature_stdev\n",
        "\n",
        "\n",
        "\n",
        "  feature_zscores = {}\n",
        "  for docs in range(len(Data)):\n",
        "      feature_zscores[docs] = {}\n",
        "      for feature in features:\n",
        "\n",
        "          # Z-score definition = (value - mean) / stddev\n",
        "          # We use intermediate variables to make the code easier to read\n",
        "          feature_val = feature_freqs[docs][feature]\n",
        "          feature_mean = corpus_features[feature][\"Mean\"]\n",
        "          feature_stdev = corpus_features[feature][\"StdDev\"]\n",
        "          feature_zscores[docs][feature] = ((feature_val-feature_mean) /\n",
        "                                              feature_stdev)\n",
        "          \n",
        "\n",
        "  # Tokenize the test case\n",
        "  testcase_tokens = nltk.word_tokenize(Test[0])\n",
        "\n",
        "  # Filter out punctuation and lowercase the tokens\n",
        "  testcase_tokens = [token.lower() for token in testcase_tokens\n",
        "                    if any(c.isalpha() for c in token)]\n",
        "\n",
        "\n",
        "  # Calculate the test case's features\n",
        "  overall = len(testcase_tokens)\n",
        "  testcase_freqs = {}\n",
        "  for feature in features:\n",
        "      presence = testcase_tokens.count(feature)\n",
        "      testcase_freqs[feature] = presence / overall\n",
        "\n",
        "  # Calculate the test case's feature z-scores\n",
        "  testcase_zscores = {}\n",
        "  for feature in features:\n",
        "      feature_val = testcase_freqs[feature]\n",
        "      feature_mean = corpus_features[feature][\"Mean\"]\n",
        "      feature_stdev = corpus_features[feature][\"StdDev\"]\n",
        "      testcase_zscores[feature] = (feature_val - feature_mean) / feature_stdev\n",
        "    #  print(\"Test case z-score for feature\", feature, \"is\", testcase_zscores[feature])\n",
        "\n",
        "\n",
        "  for docs in range(len(Data)):\n",
        "      delta = 0\n",
        "      for feature in features:\n",
        "          delta += math.fabs((testcase_zscores[feature] -\n",
        "                              feature_zscores[docs][feature]))\n",
        "    \n",
        "      delta /= len(features)\n",
        "      print( \"Delta score for candidate\", docs, \"is\", delta, \"\\n \\n \\n \\n\")\n",
        "\n",
        "      \n",
        "      Suspects_Student_X[docs] = delta\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  Top_Data = []\n",
        "  Feature_Data = []\n",
        "  Dict_names = {}\n",
        "  Names_with_documents = {}\n",
        "  Name_list = []\n",
        "  Students = 5\n",
        "\n",
        "  suspects_list = nsmallest(Students, Suspects_Student_X, key = Suspects_Student_X.get) \n",
        "\n",
        "  for docID in suspects_list:\n",
        "    Dict_names[Names[docID]] = docID\n",
        "\n",
        "  for name, ID in Dict_names.items():\n",
        "    Names_with_documents[name] = [Data[ID]]\n",
        "\n",
        "  for suspects in suspects_list:\n",
        "    Top_Data.append(preprocess(LDA_Data[suspects]))\n",
        "    Feature_Data.append(Data[suspects])\n",
        "\n",
        "\n",
        "  Name_list = list(Names_with_documents.keys())\n",
        "\n",
        "  #pprint(Names_with_documents)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #print(Top_Data)\n",
        "  dictionary_LDA = corpora.Dictionary(Top_Data)\n",
        "  dictionary_LDA.filter_extremes(no_below=0)\n",
        "  corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in Top_Data]\n",
        "  BoW = dictionary_LDA.doc2bow(LDA_Data[0])\n",
        "\n",
        "  num_topics = 5\n",
        "  lda_model = models.LdaModel(corpus, id2word=dictionary_LDA, num_topics=num_topics)\n",
        "  #x = lda_model.print_topics()\n",
        "  #x = lda_model.get_document_topics(BoW, per_word_topics=True)\n",
        "  #print(x)\n",
        "  #pprint(lda_model.print_topics())\n",
        "\n",
        "  for idx, topic in lda_model.print_topics(-1):\n",
        "   #   print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    #  print(\"\\n\")\n",
        "      x = None\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  def format_topics_sentences(ldamodel=None, corpus=corpus, texts=Data):\n",
        "      # Init output\n",
        "      sent_topics_df = pd.DataFrame()\n",
        "\n",
        "      # Get main topic in each document\n",
        "      for i, row_list in enumerate(ldamodel[corpus]):\n",
        "          row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "          # print(row)\n",
        "          row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "          # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "          for j, (topic_num, prop_topic) in enumerate(row):\n",
        "              if j == 0:  # => dominant topic\n",
        "                  wp = ldamodel.show_topic(topic_num)\n",
        "                  topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "              else:\n",
        "                  break\n",
        "      sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "      # Add original text to the end of the output\n",
        "      contents = pd.Series(texts)\n",
        "      sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "      return(sent_topics_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=Feature_Data)\n",
        "\n",
        "  # Format\n",
        "  for each_doc in LDA_Data:\n",
        "   # print(str(each_doc), \"\\n\")\n",
        "    x = None\n",
        "\n",
        "  df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "  df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "  #df_dominant_topic.head(10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  Complete_list_A = []\n",
        "  Complete_list_B = []\n",
        "  Complete_list_C = []\n",
        "  Complete_list_D = []\n",
        "  Complete_list_E = []\n",
        "\n",
        "  Complete_listWithNames = {}\n",
        "\n",
        "\n",
        "\n",
        "  index = 0\n",
        "\n",
        "  for i in range(len(Feature_Data)):\n",
        "    Feature_Data[i]\n",
        "    length_of_sentences = []\n",
        "    Mean = []\n",
        "    Standard_deviation = []\n",
        "    List = []\n",
        "    Differences = []\n",
        "    Name\n",
        "\n",
        "    if index == 0:\n",
        "      Complete_list_A.append(POS_Freq(i))\n",
        "\n",
        "    if index == 1:\n",
        "      Complete_list_B.append(POS_Freq(i))\n",
        "\n",
        "    if index == 2:\n",
        "      Complete_list_C.append(POS_Freq(i))\n",
        "\n",
        "    if index == 3:\n",
        "      Complete_list_D.append(POS_Freq(i))\n",
        "\n",
        "    if index == 4:\n",
        "      Complete_list_E.append(POS_Freq(i))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if index == 0:\n",
        "      Deviation, Mean = Standard_Deviation_Of_Sents(i)\n",
        "      Complete_list_A.append(Deviation)\n",
        "      Complete_list_A.append(Mean)\n",
        "\n",
        "      \n",
        "    if index == 1:\n",
        "      Deviation, Mean = Standard_Deviation_Of_Sents(i)\n",
        "      Complete_list_B.append(Deviation)\n",
        "      Complete_list_B.append(Mean)\n",
        "\n",
        "    if index == 2:\n",
        "      Deviation, Mean = Standard_Deviation_Of_Sents(i)\n",
        "      Complete_list_C.append(Deviation)\n",
        "      Complete_list_C.append(Mean)\n",
        "\n",
        "    if index == 3:\n",
        "      Deviation, Mean = Standard_Deviation_Of_Sents(i)\n",
        "      Complete_list_D.append(Deviation)\n",
        "      Complete_list_D.append(Mean)\n",
        "\n",
        "    if index == 4:\n",
        "      Deviation, Mean = Standard_Deviation_Of_Sents(i)\n",
        "      Complete_list_E.append(Deviation)\n",
        "      Complete_list_E.append(Mean)\n",
        "\n",
        "\n",
        "\n",
        "    if index == 0:\n",
        "      Complete_list_A.append(Vocab_richness(i))\n",
        "\n",
        "    if index == 1:\n",
        "      Complete_list_B.append(Vocab_richness(i))\n",
        "\n",
        "    if index == 2:\n",
        "      Complete_list_C.append(Vocab_richness(i))\n",
        "\n",
        "    if index == 3:\n",
        "      Complete_list_D.append(Vocab_richness(i))\n",
        "\n",
        "    if index == 4:\n",
        "      Complete_list_E.append(Vocab_richness(i)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if index == 0:\n",
        "      Complete_list_A.append(FunctionWord_freq(i))\n",
        "\n",
        "    if index == 1:\n",
        "      Complete_list_B.append(FunctionWord_freq(i))\n",
        "\n",
        "    if index == 2:\n",
        "      Complete_list_C.append(FunctionWord_freq(i))\n",
        "\n",
        "    if index == 3:\n",
        "      Complete_list_D.append(FunctionWord_freq(i))\n",
        "\n",
        "    if index == 4:\n",
        "      Complete_list_E.append(FunctionWord_freq(i)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if index == 0:\n",
        "      Complete_list_A.append(Punctuation_Freq(i))\n",
        "\n",
        "    if index == 1:\n",
        "      Complete_list_B.append(Punctuation_Freq(i))\n",
        "\n",
        "    if index == 2:\n",
        "      Complete_list_C.append(Punctuation_Freq(i))\n",
        "\n",
        "    if index == 3:\n",
        "      Complete_list_D.append(Punctuation_Freq(i))\n",
        "\n",
        "    if index == 4:\n",
        "      Complete_list_E.append(Punctuation_Freq(i)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if index == 0:\n",
        "      Complete_list_A.append(AverageSentLength(i))\n",
        "\n",
        "    if index == 1:\n",
        "      Complete_list_B.append(AverageSentLength(i))\n",
        "\n",
        "    if index == 2:\n",
        "      Complete_list_C.append(AverageSentLength(i))\n",
        "\n",
        "    if index == 3:\n",
        "      Complete_list_D.append(AverageSentLength(i))\n",
        "\n",
        "    if index == 4:\n",
        "      Complete_list_E.append(AverageSentLength(i)) \n",
        "\n",
        "\n",
        "\n",
        "    if index == 0:\n",
        "      Complete_list_A.append(NumberOfSents(i))\n",
        "\n",
        "    if index == 1:\n",
        "      Complete_list_B.append(NumberOfSents(i))\n",
        "\n",
        "    if index == 2:\n",
        "      Complete_list_C.append(NumberOfSents(i))\n",
        "\n",
        "    if index == 3:\n",
        "      Complete_list_D.append(NumberOfSents(i))\n",
        "\n",
        "    if index == 4:\n",
        "      Complete_list_E.append(NumberOfSents(i)) \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    if index == 0:\n",
        "      Complete_list_A.append(NumberOfWords(i))\n",
        "\n",
        "    if index == 1:\n",
        "      Complete_list_B.append(NumberOfWords(i))\n",
        "\n",
        "    if index == 2:\n",
        "      Complete_list_C.append(NumberOfWords(i))\n",
        "\n",
        "    if index == 3:\n",
        "      Complete_list_D.append(NumberOfWords(i))\n",
        "\n",
        "    if index == 4:\n",
        "      Complete_list_E.append(NumberOfWords(i)) \n",
        "\n",
        "    \n",
        "      \n",
        "    index = index + 1\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  Dominant_topics = []\n",
        "\n",
        "  index = 0\n",
        "\n",
        "  for i in range(len(Feature_Data)):\n",
        "    Dominant_topics.append(df_dominant_topic.at[i, \"Dominant_Topic\"])\n",
        "\n",
        "  for topics in Dominant_topics:\n",
        "\n",
        "    if index == 0:\n",
        "      Complete_list_A.append(topics)\n",
        "\n",
        "    if index == 1:\n",
        "      Complete_list_B.append(topics)\n",
        "\n",
        "    if index == 2:\n",
        "      Complete_list_C.append(topics)\n",
        "\n",
        "    if index == 3:\n",
        "      Complete_list_D.append(topics)\n",
        "\n",
        "    if index == 4:\n",
        "      Complete_list_E.append(topics)  \n",
        "    \n",
        "    index = index + 1\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #print(Complete_list_A)\n",
        "  #print(Complete_list_B)\n",
        "  #print(Complete_list_C)\n",
        "  #print(Complete_list_D)\n",
        "  #print(Complete_list_E)\n",
        "\n",
        "\n",
        "  Complete_listWithNames[Names[suspects_list[0]]] = Complete_list_A\n",
        "  Complete_listWithNames[Names[suspects_list[1]]] = Complete_list_B\n",
        "  if len(Feature_Data) >= 3:\n",
        "    Complete_listWithNames[Names[suspects_list[2]]] = Complete_list_C\n",
        "  if len(Feature_Data) == 4:\n",
        "    Complete_listWithNames[Names[suspects_list[3]]] = Complete_list_D\n",
        "  if len(Feature_Data) == 5:\n",
        "    Complete_listWithNames[Names[suspects_list[4]]] = Complete_list_E\n",
        "\n",
        "  #print(\"\\n \\n\", Complete_listWithNames, \"\\n\")\n",
        "  sentJaccard_dict = {}\n",
        "  Total_sents = []\n",
        "  AB_comparisons()\n",
        "  if len(Feature_Data) >= 3:\n",
        "    sentJaccard_dict = {}\n",
        "    Total_sents = []\n",
        "    AC_comparisons()\n",
        "  if len(Feature_Data) >= 4:\n",
        "    sentJaccard_dict = {}\n",
        "    Total_sents = []\n",
        "    AD_comparisons()\n",
        "  if len(Feature_Data) >= 5:\n",
        "      sentJaccard_dict = {}\n",
        "      Total_sents = []\n",
        "      AE_comparisons()\n",
        "\n",
        "  pprint(Accused)\n",
        "\n",
        "  Graph1()\n",
        "  Graph2()\n",
        "  Graph3()\n",
        "  Graph4()\n",
        "  SimilarityMetricsGraph()\n",
        "  SimilarityMetricsGraphWMD()\n",
        "\n",
        "  pd.set_option('display.max_colwidth',1000)\n",
        "\n",
        "  if len(Feature_Data) == 2:\n",
        "    df_data = {\"Name\" : [Name_list[0], Name_list[1]],\n",
        "    'POS Trigrams': [list(Complete_list_A[0]), list(Complete_list_B[0])],\n",
        "          'Function Words': [Complete_list_A[4], Complete_list_B[4]],\n",
        "          'Punctuation Frequency': [Complete_list_A[4], Complete_list_B[4]],\n",
        "           'Key words' : [DocA_words, DocB_words]}\n",
        "\n",
        "  if len(Feature_Data) >= 3:\n",
        "    df_data = {\"Name\" : [Name_list[0], Name_list[1], Name_list[2]],\n",
        "    'POS Trigrams': [list(Complete_list_A[0]), list(Complete_list_B[0]), list(Complete_list_C[0])],\n",
        "          'Function Words': [Complete_list_A[4], Complete_list_B[4], Complete_list_C[4]],\n",
        "          'Punctuation Frequency': [Complete_list_A[5], Complete_list_B[5], Complete_list_C[5]],\n",
        "           'Key words' : [DocA_words, DocB_words, DocC_words]}\n",
        "\n",
        "  if len(Feature_Data) >= 4:\n",
        "    df_data = {\"Name\" : [Name_list[0], Name_list[1], Name_list[2], Name_list[3]],\n",
        "    'POS Trigrams': [list(Complete_list_A[0]), list(Complete_list_B[0]), list(Complete_list_C[0]), list(Complete_list_D[0])],\n",
        "          'Function Words': [Complete_list_A[4], Complete_list_B[4], Complete_list_C[4], Complete_list_D[4]],\n",
        "          'Punctuation Frequency': [Complete_list_A[5], Complete_list_B[5], Complete_list_C[5], Complete_list_D[5]],\n",
        "           'Key words' : [DocA_words, DocB_words, DocC_words, DocD_words]}\n",
        "\n",
        "  if len(Feature_Data) == 5:\n",
        "    df_data = {\"Name\" : [Name_list[0], Name_list[1], Name_list[2], Name_list[3], Name_list[4]],\n",
        "    'POS Trigrams': [list(Complete_list_A[0]), list(Complete_list_B[0]), list(Complete_list_C[0]), list(Complete_list_D[0]), list(Complete_list_E[0])],\n",
        "          'Function Words': [Complete_list_A[4], Complete_list_B[4], Complete_list_C[4], Complete_list_D[4], Complete_list_E[4]],\n",
        "          'Punctuation Frequency': [Complete_list_A[5], Complete_list_B[5], Complete_list_C[5], Complete_list_D[5], Complete_list_E[5]],\n",
        "           'Key words' : [DocA_words, DocB_words, DocC_words, DocD_words, DocE_words]}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if len(Feature_Data) == 2:\n",
        "    lst = [['AB', AB_final, AB_count, AB_list[0], AB_list[1], AB_list[2], AB_list[3], AB_list[4], AB_list[5], AB_list[6], AB_list[7], \n",
        "            AB_list[8], AB_list[11], AB_list[-1]]]\n",
        "          \n",
        "        \n",
        "    Difference_df = pd.DataFrame(lst, columns =['Student A to Student X', 'Final Result', 'Total Score',\n",
        "                                    'Common POS Trigrams ', \n",
        "                                    'Standard Deviation Difference ', \n",
        "                                    'Mean Length of Sentences Difference',\n",
        "                                    'Unique Words Difference ',\n",
        "                                    'Common Function Words ',\n",
        "                                    'Common Punctuation', \n",
        "                                    'Average Sentence Length Difference ',                  \n",
        "                                    'Number of Sentences Difference ',\n",
        "                                    'Number of Words Difference ',\n",
        "                                    'Suspicious Sentences (Using Jaccard)',\n",
        "                                    'Topic (True or False)'\n",
        "                                    ])\n",
        "  \n",
        "\n",
        "  if len(Feature_Data) == 3:\n",
        "    lst = [['AB', AB_final, AB_count, AB_list[0], AB_list[1], AB_list[2], AB_list[3], AB_list[4], AB_list[5], AB_list[6], AB_list[7], \n",
        "            AB_list[8], AB_list[11], AB_list[-1]],\n",
        "           \n",
        "           ['AC', AC_final, AC_count, AC_list[0], AC_list[1], AC_list[2], AC_list[3], AC_list[4], AC_list[5], AC_list[6], AC_list[7], \n",
        "            AC_list[8], AC_list[11], AC_list[-1]]]\n",
        "\n",
        "          \n",
        "        \n",
        "    Difference_df = pd.DataFrame(lst, columns =['Student A to Student X', 'Final Result', 'Total Score',\n",
        "                                    'Common POS Trigrams ', \n",
        "                                    'Standard Deviation Difference ', \n",
        "                                    'Mean Length of Sentences Difference',\n",
        "                                    'Unique Words Difference ',\n",
        "                                    'Common Function Words ',\n",
        "                                    'Common Punctuation', \n",
        "                                    'Average Sentence Length Difference ',                  \n",
        "                                    'Number of Sentences Difference ',\n",
        "                                    'Number of Words Difference ',\n",
        "                                    'Suspicious Sentences (Using Jaccard)',\n",
        "                                    'Topic (True or False)'\n",
        "                                    ])\n",
        "\n",
        "\n",
        "\n",
        "  if len(Feature_Data) == 4:\n",
        "    lst = [['AB', AB_final, AB_count, AB_list[0], AB_list[1], AB_list[2], AB_list[3], AB_list[4], AB_list[5], AB_list[6], AB_list[7], \n",
        "            AB_list[8], AB_list[11], AB_list[-1]], \n",
        "          \n",
        "          ['AC', AC_final, AC_count, AC_list[0], AC_list[1], AC_list[2], AC_list[3], AC_list[4], AC_list[5], AC_list[6], AC_list[7], \n",
        "            AC_list[8],AC_list[11], AC_list[-1]],\n",
        "          \n",
        "          ['AD', AD_final, AD_count, AD_list[0], AD_list[1], AD_list[2], AD_list[3], AD_list[4], AD_list[5], AD_list[6], AD_list[7], \n",
        "            AD_list[8], AD_list[11], AD_list[-1]]]\n",
        "          \n",
        "        \n",
        "    Difference_df = pd.DataFrame(lst, columns =['Student A to Student X', 'Final Result', 'Total Score ',\n",
        "                                    'Common POS Trigrams ', \n",
        "                                    'Standard Deviation Difference ', \n",
        "                                    'Mean Length of Sentences Difference',\n",
        "                                    'Unique Words Difference ',\n",
        "                                    'Common Function Words ',\n",
        "                                    'Common Punctuation', \n",
        "                                    'Average Sentence Length Difference ',                  \n",
        "                                    'Number of Sentences Difference ',\n",
        "                                    'Number of Words Difference ',\n",
        "                                    'Suspicious Sentences (Using Jaccard)',\n",
        "                                    'Topic (True or False)'\n",
        "                                    ])\n",
        "\n",
        "\n",
        "\n",
        "  if len(Feature_Data) == 5:\n",
        "    lst = [['AB', AB_final, AB_count, AB_list[0], AB_list[1], AB_list[2], AB_list[3], AB_list[4], AB_list[5], AB_list[6], AB_list[7], \n",
        "            AB_list[8], AB_list[11], AB_list[-1]], \n",
        "          \n",
        "          ['AC', AC_final, AC_count, AC_list[0], AC_list[1], AC_list[2], AC_list[3], AC_list[4], AC_list[5], AC_list[6], AC_list[7], \n",
        "            AC_list[8],AC_list[11], AC_list[-1]],\n",
        "          \n",
        "          ['AD', AD_final, AD_count, AD_list[0], AD_list[1], AD_list[2], AD_list[3], AD_list[4], AD_list[5], AD_list[6], AD_list[7], \n",
        "            AD_list[8], AD_list[11], AD_list[-1]],\n",
        "          \n",
        "          ['AE', AE_final, AE_count, AE_list[0], AE_list[1], AE_list[2], AE_list[3], AE_list[4], AE_list[5], AE_list[6], AE_list[7], \n",
        "            AE_list[8], AE_list[11], AE_list[-1]]]\n",
        "          \n",
        "        \n",
        "    Difference_df = pd.DataFrame(lst, columns =['Student A to Student X', 'Final Result', 'Total Score ',\n",
        "                                    'Common POS Trigrams ', \n",
        "                                    'Standard Deviation Difference ', \n",
        "                                    'Mean Length of Sentences Difference',\n",
        "                                    'Unique Words Difference ',\n",
        "                                    'Common Function Words ',\n",
        "                                    'Common Punctuation', \n",
        "                                    'Average Sentence Length Difference ',                  \n",
        "                                    'Number of Sentences Difference ',\n",
        "                                    'Number of Words Difference ',\n",
        "                                    'Suspicious Sentences (Using Jaccard)',\n",
        "                                    'Topic (True or False)'\n",
        "                                    ])\n",
        "\n",
        "\n",
        "  display(pd.DataFrame(df_data, columns = ['Name','POS Trigrams','Function Words',\n",
        "                                           'Punctuation Frequency', 'Key words']))\n",
        "  display(Difference_df)\n",
        "\n",
        "\n",
        "  data_accused = {\"Names 1\" : list(Accused.keys()) , \n",
        "                \"Names 2\" : list(Accused.values())}\n",
        "  print(\" \\n \\n \")\n",
        "  display(pd.DataFrame(data_accused, columns = [\"Names 1\", \"Names 2\"]))\n",
        "  print(\" \\n \\n \")\n",
        "\n",
        "  Name_tags = Name_tags + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw-MOEMw03Oo"
      },
      "source": [
        "# Results Down Here"
      ]
    }
  ]
}